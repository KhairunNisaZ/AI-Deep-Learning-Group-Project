{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","mount_file_id":"1LUyxfEA6VCE3Mc1L7MPdBokhfmI54zIS","authorship_tag":"ABX9TyPBzNJeo7rUZEkhNFzPmCkW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Data Augmentation with Generative Adversarial Network (GAN)\n","Generative Adversarial Network (GAN) adalah salah satu metode augmentasi data memanfaatkan deep learning. GAN terdiri dari 2 network, yaitu Generator Network dan Discriminator Network. Generator bertugas untuk membuat data sintetik yang menyerupai data asli. Discriminator bertugas sebagai classifier yang memisahkan data asli dan data sintetik."],"metadata":{"id":"KfY_owP3tk6s"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import Dataset\n","\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"ptg8-vV_Duzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mendefinisikan class Discriminator\n","# Berperan sebagai classifier untuk memisahkan data asli dan sintetik.\n","# Output layer menggunakan Sigmoid activation function agar memiliki hasil di antara 0 sampai 1\n","# Setiap Dense Layer juga memiliki LeakyReLU activation function. Pemilihan LeakyReLU didasarkan\n","# pada beberapa referensi model GAN yang juga menggunakan activation function yang sama.\n","\n","class Discriminator(nn.Module):\n","  def __init__(self, n_features):\n","    super().__init__()\n","    self.disc = nn.Sequential(\n","        nn.Linear(n_features, 50),\n","        nn.LeakyReLU(.02),\n","\n","        nn.Linear(50, 25),\n","        nn.LeakyReLU(0.02),\n","        nn.Dropout(.8),\n","\n","        nn.Linear(25, 1),\n","        nn.Sigmoid(),\n","    )\n","\n","  def forward(self, X):\n","    return self.disc(X)\n","\n","# Mendefinisikan class Generator\n","# Bertugas untuk menghasilkan data sintetik semirip mungkin dengan data asli.\n","# Setiap Dense Layer memiliki LeakyReLU activation function. Output layer menggunakan Tanh. Pemilihan layer didasarkan\n","# pada beberapa referensi model GAN yang juga menggunakan activation function yang sama.\n","\n","class Generator(nn.Module):\n","  def __init__(self, latent_noise, n_features):\n","    super().__init__()\n","    self.gen = nn.Sequential(\n","        nn.Linear(latent_noise, 10), #1024\n","        nn.LeakyReLU(.02), #0.02\n","\n","        nn.Linear(10, 50), #1024, 512\n","        nn.LeakyReLU(.02), #0.02\n","\n","        nn.Linear(50, n_features), #512, 14\n","        nn.Tanh(),\n","    )\n","\n","  def forward(self, X):\n","    return self.gen(X)\n","\n","\n","# inisiasi class GAN\n","class GAN(nn.Module):\n","  def __init__(self, latent_noise, generator_n_output):\n","    super(GAN, self).__init__()\n","    self.generator = Generator(latent_noise, n_features)\n","    self.discriminator = Discriminator(generator_n_output)\n","\n","\n","  def forward(self, X):\n","    generated_data = self.generator(X)\n","    discriminator_output = self.discriminator(generated_data)\n","    return generated_data, discriminator_output"],"metadata":{"id":"ahHx8X8sEJVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train one epoch\n","# tujuan training adalah untuk menghasilkan error discriminator sebesar mungkin sebab\n","# itu berarti discriminator semakin gagal dalam melakukan klasifikasi data asli dan sintetik\n","\n","def train(data_loader, generator, discriminator, gan_optim, disc_optim, criterion):\n","  device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","  generator.to(device)\n","  discriminator.to(device)\n","  discriminator_loss = 0\n","  generator_loss = 0\n","\n","  for real_data in data_loader:\n","    real_data = real_data.to(device)\n","\n","    disc_optim.zero_grad()\n","\n","    batch_size = real_data.size(0)\n","    noise = torch.randn(batch_size, latent_noise).to(device)\n","    generated_data = generator(noise)\n","\n","    real_labels = torch.ones(batch_size, 1).to(device)\n","    fake_labels = torch.zeros(batch_size, 1).to(device)\n","    real_loss = criterion(discriminator(real_data), real_labels)\n","    fake_loss = criterion(discriminator(generated_data.detach()), fake_labels)\n","    discriminator_loss = real_loss + fake_loss\n","\n","    # discriminator loss dipropagasikan untuk mengkoreksi weight pada hidden layer\n","    discriminator_loss.backward()\n","    disc_optim.step()\n","\n","    gan_optim.zero_grad()\n","\n","    noise = torch.randn(batch_size, latent_noise).to(device)\n","    generated_data = generator(noise)\n","\n","    # generator loss merupakan loss dari discriminator yang dipropagasikan ke hidden layer Generator\n","    generator_loss = criterion(discriminator(generated_data), real_labels)\n","\n","    generator_loss.backward()\n","    gan_optim.step()\n","\n","\n","  return discriminator_loss, generator_loss"],"metadata":{"id":"_CEIO8c_Kj-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set random seed\n","torch.manual_seed(251003)\n","\n","# Hyperparameters\n","latent_noise = 13\n","n_features = 13\n","batch_size = 32\n","num_epochs = 1000\n","lr = 3e-7"],"metadata":{"id":"JBHbmMh2Fb7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gan = GAN(latent_noise, n_features)\n","\n","# Mendefinisikan loss function menggunakan Binary Crossentropy Loss\n","criterion = nn.BCELoss()\n","\n","# Mendefinisikan Teknik Propagasi atau Optimisasi dengan Adam\n","gan_optim = optim.Adam(gan.generator.parameters(), lr=lr)\n","disc_optim = optim.Adam(gan.discriminator.parameters(), lr=lr)"],"metadata":{"id":"o0cAHF17Gp1s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Preprocessing\n","Bagian penting dalam data preprocessing adalah memastikan tidak ada value null atau NaN di dalam dataset. Data juga dinormalkan agar memiliki range value antara 0 sampai 1"],"metadata":{"id":"jComoypjRALp"}},{"cell_type":"code","source":["pumpkin_data = pd.read_excel('/content/drive/MyDrive/Datasets/Pumpkin_Seeds_Dataset.xlsx')\n","\n","class_map = {'Çerçevelik': 0, 'Ürgüp Sivrisi':1}\n","pumpkin_data['Class'] = pumpkin_data['Class'].replace(class_map)\n","\n","pumpkin_data.describe()"],"metadata":{"id":"OUYKNeh1HSKc","executionInfo":{"status":"ok","timestamp":1686539669038,"user_tz":-420,"elapsed":644,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"colab":{"base_uri":"https://localhost:8080/","height":300},"outputId":"fec54197-2480-4a44-9a21-7b6759e47e5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                Area    Perimeter  Major_Axis_Length  Minor_Axis_Length  \\\n","count    2500.000000  2500.000000        2500.000000        2500.000000   \n","mean    80658.220800  1130.279015         456.601840         225.794921   \n","std     13664.510228   109.256418          56.235704          23.297245   \n","min     47939.000000   868.485000         320.844600         152.171800   \n","25%     70765.000000  1048.829750         414.957850         211.245925   \n","50%     79076.000000  1123.672000         449.496600         224.703100   \n","75%     89757.500000  1203.340500         492.737650         240.672875   \n","max    136574.000000  1559.450000         661.911300         305.818000   \n","\n","         Convex_Area  Equiv_Diameter  Eccentricity     Solidity       Extent  \\\n","count    2500.000000     2500.000000   2500.000000  2500.000000  2500.000000   \n","mean    81508.084400      319.334230      0.860879     0.989492     0.693205   \n","std     13764.092788       26.891920      0.045167     0.003494     0.060914   \n","min     48366.000000      247.058400      0.492100     0.918600     0.468000   \n","25%     71512.000000      300.167975      0.831700     0.988300     0.658900   \n","50%     79872.000000      317.305350      0.863700     0.990300     0.713050   \n","75%     90797.750000      338.057375      0.897025     0.991500     0.740225   \n","max    138384.000000      417.002900      0.948100     0.994400     0.829600   \n","\n","         Roundness  Aspect_Ration  Compactness      Class  \n","count  2500.000000    2500.000000  2500.000000  2500.0000  \n","mean      0.791533       2.041702     0.704121     0.4800  \n","std       0.055924       0.315997     0.053067     0.4997  \n","min       0.554600       1.148700     0.560800     0.0000  \n","25%       0.751900       1.801050     0.663475     0.0000  \n","50%       0.797750       1.984200     0.707700     0.0000  \n","75%       0.834325       2.262075     0.743500     1.0000  \n","max       0.939600       3.144400     0.904900     1.0000  "],"text/html":["\n","  <div id=\"df-e1abc9ea-3198-4b40-ad09-408a446eaec9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Area</th>\n","      <th>Perimeter</th>\n","      <th>Major_Axis_Length</th>\n","      <th>Minor_Axis_Length</th>\n","      <th>Convex_Area</th>\n","      <th>Equiv_Diameter</th>\n","      <th>Eccentricity</th>\n","      <th>Solidity</th>\n","      <th>Extent</th>\n","      <th>Roundness</th>\n","      <th>Aspect_Ration</th>\n","      <th>Compactness</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.0000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>80658.220800</td>\n","      <td>1130.279015</td>\n","      <td>456.601840</td>\n","      <td>225.794921</td>\n","      <td>81508.084400</td>\n","      <td>319.334230</td>\n","      <td>0.860879</td>\n","      <td>0.989492</td>\n","      <td>0.693205</td>\n","      <td>0.791533</td>\n","      <td>2.041702</td>\n","      <td>0.704121</td>\n","      <td>0.4800</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>13664.510228</td>\n","      <td>109.256418</td>\n","      <td>56.235704</td>\n","      <td>23.297245</td>\n","      <td>13764.092788</td>\n","      <td>26.891920</td>\n","      <td>0.045167</td>\n","      <td>0.003494</td>\n","      <td>0.060914</td>\n","      <td>0.055924</td>\n","      <td>0.315997</td>\n","      <td>0.053067</td>\n","      <td>0.4997</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>47939.000000</td>\n","      <td>868.485000</td>\n","      <td>320.844600</td>\n","      <td>152.171800</td>\n","      <td>48366.000000</td>\n","      <td>247.058400</td>\n","      <td>0.492100</td>\n","      <td>0.918600</td>\n","      <td>0.468000</td>\n","      <td>0.554600</td>\n","      <td>1.148700</td>\n","      <td>0.560800</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>70765.000000</td>\n","      <td>1048.829750</td>\n","      <td>414.957850</td>\n","      <td>211.245925</td>\n","      <td>71512.000000</td>\n","      <td>300.167975</td>\n","      <td>0.831700</td>\n","      <td>0.988300</td>\n","      <td>0.658900</td>\n","      <td>0.751900</td>\n","      <td>1.801050</td>\n","      <td>0.663475</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>79076.000000</td>\n","      <td>1123.672000</td>\n","      <td>449.496600</td>\n","      <td>224.703100</td>\n","      <td>79872.000000</td>\n","      <td>317.305350</td>\n","      <td>0.863700</td>\n","      <td>0.990300</td>\n","      <td>0.713050</td>\n","      <td>0.797750</td>\n","      <td>1.984200</td>\n","      <td>0.707700</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>89757.500000</td>\n","      <td>1203.340500</td>\n","      <td>492.737650</td>\n","      <td>240.672875</td>\n","      <td>90797.750000</td>\n","      <td>338.057375</td>\n","      <td>0.897025</td>\n","      <td>0.991500</td>\n","      <td>0.740225</td>\n","      <td>0.834325</td>\n","      <td>2.262075</td>\n","      <td>0.743500</td>\n","      <td>1.0000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>136574.000000</td>\n","      <td>1559.450000</td>\n","      <td>661.911300</td>\n","      <td>305.818000</td>\n","      <td>138384.000000</td>\n","      <td>417.002900</td>\n","      <td>0.948100</td>\n","      <td>0.994400</td>\n","      <td>0.829600</td>\n","      <td>0.939600</td>\n","      <td>3.144400</td>\n","      <td>0.904900</td>\n","      <td>1.0000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1abc9ea-3198-4b40-ad09-408a446eaec9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e1abc9ea-3198-4b40-ad09-408a446eaec9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e1abc9ea-3198-4b40-ad09-408a446eaec9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# normalisasi data\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","scaler.fit(pumpkin_data)\n","pumpkin_data_norm = scaler.transform(pumpkin_data)\n","df = pumpkin_data_norm\n","df = df.astype('float32')\n","df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LCLT52HWQ0B1","executionInfo":{"status":"ok","timestamp":1686539672684,"user_tz":-420,"elapsed":1552,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"55241aeb-5a9e-498d-8947-481136f6f295"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2500, 13)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Mendefinisikan class Dataset library Torch\n","\n","class TabularDataset(Dataset):\n","  def __init__(self, data):\n","    self.data = data\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, idx):\n","    sample = self.data[idx]\n","    return sample"],"metadata":{"id":"5sC87b7aSAcw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = TabularDataset(df)"],"metadata":{"id":"8csovBJISg_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"tiX2X_zcJmFj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Start training model"],"metadata":{"id":"_tPQAfk4Q8fy"}},{"cell_type":"code","source":["# Memulai proses train model\n","\n","for epoch in range(num_epochs):\n","  discriminator_loss, generator_loss = train(data_loader, gan.generator, gan.discriminator, gan_optim, disc_optim, criterion)\n","  print(f\"Epoch: {epoch+1} | Generator Loss: {generator_loss} | Discriminator Loss: {discriminator_loss}\")\n","  if generator_loss == 100:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZt1xmzoH0sz","executionInfo":{"status":"ok","timestamp":1686540020941,"user_tz":-420,"elapsed":335007,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"82cd83ca-2b0d-4a40-9db5-ed071d48f135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Generator Loss: 0.7471600770950317 | Discriminator Loss: 1.4251220226287842\n","Epoch: 2 | Generator Loss: 0.7043226957321167 | Discriminator Loss: 1.3623614311218262\n","Epoch: 3 | Generator Loss: 0.686579704284668 | Discriminator Loss: 1.482797384262085\n","Epoch: 4 | Generator Loss: 0.6987943649291992 | Discriminator Loss: 1.36784827709198\n","Epoch: 5 | Generator Loss: 0.7196148633956909 | Discriminator Loss: 1.5038294792175293\n","Epoch: 6 | Generator Loss: 0.7213377952575684 | Discriminator Loss: 1.3956871032714844\n","Epoch: 7 | Generator Loss: 0.7780773043632507 | Discriminator Loss: 1.3711570501327515\n","Epoch: 8 | Generator Loss: 0.7424606084823608 | Discriminator Loss: 1.468957543373108\n","Epoch: 9 | Generator Loss: 0.6931590437889099 | Discriminator Loss: 1.3801465034484863\n","Epoch: 10 | Generator Loss: 0.7384734153747559 | Discriminator Loss: 1.4347805976867676\n","Epoch: 11 | Generator Loss: 0.7559648752212524 | Discriminator Loss: 1.4036240577697754\n","Epoch: 12 | Generator Loss: 0.7713848948478699 | Discriminator Loss: 1.4017174243927002\n","Epoch: 13 | Generator Loss: 0.7526363134384155 | Discriminator Loss: 1.3549087047576904\n","Epoch: 14 | Generator Loss: 0.7618350386619568 | Discriminator Loss: 1.4137396812438965\n","Epoch: 15 | Generator Loss: 0.7365661859512329 | Discriminator Loss: 1.374763011932373\n","Epoch: 16 | Generator Loss: 0.7556784152984619 | Discriminator Loss: 1.3486545085906982\n","Epoch: 17 | Generator Loss: 0.7437050342559814 | Discriminator Loss: 1.4197343587875366\n","Epoch: 18 | Generator Loss: 0.7353688478469849 | Discriminator Loss: 1.337385892868042\n","Epoch: 19 | Generator Loss: 0.7245200872421265 | Discriminator Loss: 1.4080398082733154\n","Epoch: 20 | Generator Loss: 0.7420452833175659 | Discriminator Loss: 1.4344308376312256\n","Epoch: 21 | Generator Loss: 0.7455341219902039 | Discriminator Loss: 1.353182077407837\n","Epoch: 22 | Generator Loss: 0.7955719828605652 | Discriminator Loss: 1.3575489521026611\n","Epoch: 23 | Generator Loss: 0.7261726260185242 | Discriminator Loss: 1.4198424816131592\n","Epoch: 24 | Generator Loss: 0.7396519184112549 | Discriminator Loss: 1.3332688808441162\n","Epoch: 25 | Generator Loss: 0.74916672706604 | Discriminator Loss: 1.3333954811096191\n","Epoch: 26 | Generator Loss: 0.7241951823234558 | Discriminator Loss: 1.286441445350647\n","Epoch: 27 | Generator Loss: 0.7205913066864014 | Discriminator Loss: 1.4582793712615967\n","Epoch: 28 | Generator Loss: 0.7181198596954346 | Discriminator Loss: 1.3380241394042969\n","Epoch: 29 | Generator Loss: 0.7489232420921326 | Discriminator Loss: 1.4220709800720215\n","Epoch: 30 | Generator Loss: 0.7183398008346558 | Discriminator Loss: 1.3495159149169922\n","Epoch: 31 | Generator Loss: 0.7375150322914124 | Discriminator Loss: 1.3649060726165771\n","Epoch: 32 | Generator Loss: 0.768409252166748 | Discriminator Loss: 1.341900110244751\n","Epoch: 33 | Generator Loss: 0.7347332239151001 | Discriminator Loss: 1.4325062036514282\n","Epoch: 34 | Generator Loss: 0.762703001499176 | Discriminator Loss: 1.4488601684570312\n","Epoch: 35 | Generator Loss: 0.7289619445800781 | Discriminator Loss: 1.4029316902160645\n","Epoch: 36 | Generator Loss: 0.7414404153823853 | Discriminator Loss: 1.3030118942260742\n","Epoch: 37 | Generator Loss: 0.7156013250350952 | Discriminator Loss: 1.413255214691162\n","Epoch: 38 | Generator Loss: 0.728657603263855 | Discriminator Loss: 1.4405549764633179\n","Epoch: 39 | Generator Loss: 0.7393810153007507 | Discriminator Loss: 1.4029312133789062\n","Epoch: 40 | Generator Loss: 0.7656283974647522 | Discriminator Loss: 1.338794231414795\n","Epoch: 41 | Generator Loss: 0.7186738848686218 | Discriminator Loss: 1.44679856300354\n","Epoch: 42 | Generator Loss: 0.7417030930519104 | Discriminator Loss: 1.3170791864395142\n","Epoch: 43 | Generator Loss: 0.7551023960113525 | Discriminator Loss: 1.3875652551651\n","Epoch: 44 | Generator Loss: 0.7016510963439941 | Discriminator Loss: 1.3990278244018555\n","Epoch: 45 | Generator Loss: 0.7105860710144043 | Discriminator Loss: 1.3527841567993164\n","Epoch: 46 | Generator Loss: 0.7679031491279602 | Discriminator Loss: 1.423742413520813\n","Epoch: 47 | Generator Loss: 0.7259833812713623 | Discriminator Loss: 1.3641440868377686\n","Epoch: 48 | Generator Loss: 0.7060913443565369 | Discriminator Loss: 1.3921144008636475\n","Epoch: 49 | Generator Loss: 0.7578025460243225 | Discriminator Loss: 1.3975684642791748\n","Epoch: 50 | Generator Loss: 0.716076135635376 | Discriminator Loss: 1.3666133880615234\n","Epoch: 51 | Generator Loss: 0.7417431473731995 | Discriminator Loss: 1.397257685661316\n","Epoch: 52 | Generator Loss: 0.740634560585022 | Discriminator Loss: 1.4729143381118774\n","Epoch: 53 | Generator Loss: 0.7546342015266418 | Discriminator Loss: 1.502958059310913\n","Epoch: 54 | Generator Loss: 0.722152054309845 | Discriminator Loss: 1.339160442352295\n","Epoch: 55 | Generator Loss: 0.7683668732643127 | Discriminator Loss: 1.3175129890441895\n","Epoch: 56 | Generator Loss: 0.7442203164100647 | Discriminator Loss: 1.3227620124816895\n","Epoch: 57 | Generator Loss: 0.7429423928260803 | Discriminator Loss: 1.4077391624450684\n","Epoch: 58 | Generator Loss: 0.7242845296859741 | Discriminator Loss: 1.5029746294021606\n","Epoch: 59 | Generator Loss: 0.7229829430580139 | Discriminator Loss: 1.3914542198181152\n","Epoch: 60 | Generator Loss: 0.7141251564025879 | Discriminator Loss: 1.2802956104278564\n","Epoch: 61 | Generator Loss: 0.7358141541481018 | Discriminator Loss: 1.3917124271392822\n","Epoch: 62 | Generator Loss: 0.724307656288147 | Discriminator Loss: 1.4895656108856201\n","Epoch: 63 | Generator Loss: 0.7641555070877075 | Discriminator Loss: 1.2947288751602173\n","Epoch: 64 | Generator Loss: 0.7396210432052612 | Discriminator Loss: 1.3684468269348145\n","Epoch: 65 | Generator Loss: 0.7254139184951782 | Discriminator Loss: 1.4498543739318848\n","Epoch: 66 | Generator Loss: 0.769709587097168 | Discriminator Loss: 1.4340147972106934\n","Epoch: 67 | Generator Loss: 0.7537950277328491 | Discriminator Loss: 1.3333709239959717\n","Epoch: 68 | Generator Loss: 0.7278782725334167 | Discriminator Loss: 1.357289433479309\n","Epoch: 69 | Generator Loss: 0.706066370010376 | Discriminator Loss: 1.3241695165634155\n","Epoch: 70 | Generator Loss: 0.7332677841186523 | Discriminator Loss: 1.3552663326263428\n","Epoch: 71 | Generator Loss: 0.7495414018630981 | Discriminator Loss: 1.34696626663208\n","Epoch: 72 | Generator Loss: 0.7567125558853149 | Discriminator Loss: 1.3491946458816528\n","Epoch: 73 | Generator Loss: 0.721235454082489 | Discriminator Loss: 1.4151270389556885\n","Epoch: 74 | Generator Loss: 0.7321563959121704 | Discriminator Loss: 1.353297233581543\n","Epoch: 75 | Generator Loss: 0.7177388668060303 | Discriminator Loss: 1.3688099384307861\n","Epoch: 76 | Generator Loss: 0.7593995928764343 | Discriminator Loss: 1.4136974811553955\n","Epoch: 77 | Generator Loss: 0.710155189037323 | Discriminator Loss: 1.3934612274169922\n","Epoch: 78 | Generator Loss: 0.7397473454475403 | Discriminator Loss: 1.34159517288208\n","Epoch: 79 | Generator Loss: 0.7784184217453003 | Discriminator Loss: 1.3696353435516357\n","Epoch: 80 | Generator Loss: 0.7603079676628113 | Discriminator Loss: 1.3262836933135986\n","Epoch: 81 | Generator Loss: 0.7751923203468323 | Discriminator Loss: 1.3336024284362793\n","Epoch: 82 | Generator Loss: 0.7284064292907715 | Discriminator Loss: 1.2773361206054688\n","Epoch: 83 | Generator Loss: 0.7090264558792114 | Discriminator Loss: 1.4186577796936035\n","Epoch: 84 | Generator Loss: 0.7085418105125427 | Discriminator Loss: 1.3647873401641846\n","Epoch: 85 | Generator Loss: 0.7658373117446899 | Discriminator Loss: 1.386638879776001\n","Epoch: 86 | Generator Loss: 0.7785494327545166 | Discriminator Loss: 1.3488187789916992\n","Epoch: 87 | Generator Loss: 0.7436553239822388 | Discriminator Loss: 1.4580566883087158\n","Epoch: 88 | Generator Loss: 0.7569498419761658 | Discriminator Loss: 1.335265874862671\n","Epoch: 89 | Generator Loss: 0.7303740382194519 | Discriminator Loss: 1.4083809852600098\n","Epoch: 90 | Generator Loss: 0.7350796461105347 | Discriminator Loss: 1.3748514652252197\n","Epoch: 91 | Generator Loss: 0.7913835048675537 | Discriminator Loss: 1.357661247253418\n","Epoch: 92 | Generator Loss: 0.7467185854911804 | Discriminator Loss: 1.4067615270614624\n","Epoch: 93 | Generator Loss: 0.7529041767120361 | Discriminator Loss: 1.3180129528045654\n","Epoch: 94 | Generator Loss: 0.7194044589996338 | Discriminator Loss: 1.4233450889587402\n","Epoch: 95 | Generator Loss: 0.7250676155090332 | Discriminator Loss: 1.3688757419586182\n","Epoch: 96 | Generator Loss: 0.7711779475212097 | Discriminator Loss: 1.4036293029785156\n","Epoch: 97 | Generator Loss: 0.7247140407562256 | Discriminator Loss: 1.3294868469238281\n","Epoch: 98 | Generator Loss: 0.7699010372161865 | Discriminator Loss: 1.3404278755187988\n","Epoch: 99 | Generator Loss: 0.7684475183486938 | Discriminator Loss: 1.3897778987884521\n","Epoch: 100 | Generator Loss: 0.706217885017395 | Discriminator Loss: 1.3911066055297852\n","Epoch: 101 | Generator Loss: 0.756197452545166 | Discriminator Loss: 1.4376428127288818\n","Epoch: 102 | Generator Loss: 0.7241131067276001 | Discriminator Loss: 1.2283766269683838\n","Epoch: 103 | Generator Loss: 0.7020168900489807 | Discriminator Loss: 1.3712403774261475\n","Epoch: 104 | Generator Loss: 0.7521620392799377 | Discriminator Loss: 1.3352466821670532\n","Epoch: 105 | Generator Loss: 0.7765835523605347 | Discriminator Loss: 1.3513073921203613\n","Epoch: 106 | Generator Loss: 0.7186546325683594 | Discriminator Loss: 1.389944076538086\n","Epoch: 107 | Generator Loss: 0.751272439956665 | Discriminator Loss: 1.3094377517700195\n","Epoch: 108 | Generator Loss: 0.7307922840118408 | Discriminator Loss: 1.4043092727661133\n","Epoch: 109 | Generator Loss: 0.748849093914032 | Discriminator Loss: 1.3182668685913086\n","Epoch: 110 | Generator Loss: 0.7113851308822632 | Discriminator Loss: 1.376032829284668\n","Epoch: 111 | Generator Loss: 0.7339774370193481 | Discriminator Loss: 1.3722432851791382\n","Epoch: 112 | Generator Loss: 0.7095841765403748 | Discriminator Loss: 1.5249508619308472\n","Epoch: 113 | Generator Loss: 0.7323940992355347 | Discriminator Loss: 1.4079737663269043\n","Epoch: 114 | Generator Loss: 0.7157444357872009 | Discriminator Loss: 1.3864781856536865\n","Epoch: 115 | Generator Loss: 0.7259976863861084 | Discriminator Loss: 1.383800745010376\n","Epoch: 116 | Generator Loss: 0.7082656621932983 | Discriminator Loss: 1.3525779247283936\n","Epoch: 117 | Generator Loss: 0.7664015293121338 | Discriminator Loss: 1.3352105617523193\n","Epoch: 118 | Generator Loss: 0.7505197525024414 | Discriminator Loss: 1.3407881259918213\n","Epoch: 119 | Generator Loss: 0.7269459962844849 | Discriminator Loss: 1.4342044591903687\n","Epoch: 120 | Generator Loss: 0.7335062623023987 | Discriminator Loss: 1.314079761505127\n","Epoch: 121 | Generator Loss: 0.7206185460090637 | Discriminator Loss: 1.3892028331756592\n","Epoch: 122 | Generator Loss: 0.7467104196548462 | Discriminator Loss: 1.4198107719421387\n","Epoch: 123 | Generator Loss: 0.7061570882797241 | Discriminator Loss: 1.3444905281066895\n","Epoch: 124 | Generator Loss: 0.7489163875579834 | Discriminator Loss: 1.3102192878723145\n","Epoch: 125 | Generator Loss: 0.7483031153678894 | Discriminator Loss: 1.32265043258667\n","Epoch: 126 | Generator Loss: 0.7710906267166138 | Discriminator Loss: 1.3276258707046509\n","Epoch: 127 | Generator Loss: 0.7295007705688477 | Discriminator Loss: 1.4149305820465088\n","Epoch: 128 | Generator Loss: 0.7198265790939331 | Discriminator Loss: 1.3414099216461182\n","Epoch: 129 | Generator Loss: 0.7902419567108154 | Discriminator Loss: 1.3762720823287964\n","Epoch: 130 | Generator Loss: 0.758273184299469 | Discriminator Loss: 1.3996303081512451\n","Epoch: 131 | Generator Loss: 0.6739374399185181 | Discriminator Loss: 1.344862937927246\n","Epoch: 132 | Generator Loss: 0.6973304748535156 | Discriminator Loss: 1.3112719058990479\n","Epoch: 133 | Generator Loss: 0.7214234471321106 | Discriminator Loss: 1.3426939249038696\n","Epoch: 134 | Generator Loss: 0.7448307871818542 | Discriminator Loss: 1.3363218307495117\n","Epoch: 135 | Generator Loss: 0.7279539704322815 | Discriminator Loss: 1.462310791015625\n","Epoch: 136 | Generator Loss: 0.713912308216095 | Discriminator Loss: 1.3225462436676025\n","Epoch: 137 | Generator Loss: 0.7417638301849365 | Discriminator Loss: 1.2706782817840576\n","Epoch: 138 | Generator Loss: 0.7485010623931885 | Discriminator Loss: 1.3851637840270996\n","Epoch: 139 | Generator Loss: 0.767483115196228 | Discriminator Loss: 1.3893022537231445\n","Epoch: 140 | Generator Loss: 0.7563042044639587 | Discriminator Loss: 1.3585363626480103\n","Epoch: 141 | Generator Loss: 0.6970431804656982 | Discriminator Loss: 1.3137516975402832\n","Epoch: 142 | Generator Loss: 0.7525027990341187 | Discriminator Loss: 1.3590826988220215\n","Epoch: 143 | Generator Loss: 0.7329235076904297 | Discriminator Loss: 1.3872601985931396\n","Epoch: 144 | Generator Loss: 0.7180556058883667 | Discriminator Loss: 1.3300659656524658\n","Epoch: 145 | Generator Loss: 0.8035134077072144 | Discriminator Loss: 1.3216912746429443\n","Epoch: 146 | Generator Loss: 0.7298313975334167 | Discriminator Loss: 1.308624267578125\n","Epoch: 147 | Generator Loss: 0.7163996696472168 | Discriminator Loss: 1.3505295515060425\n","Epoch: 148 | Generator Loss: 0.7230689525604248 | Discriminator Loss: 1.4104573726654053\n","Epoch: 149 | Generator Loss: 0.7245809435844421 | Discriminator Loss: 1.4113576412200928\n","Epoch: 150 | Generator Loss: 0.7350532412528992 | Discriminator Loss: 1.3341481685638428\n","Epoch: 151 | Generator Loss: 0.7590755224227905 | Discriminator Loss: 1.3568358421325684\n","Epoch: 152 | Generator Loss: 0.7299619913101196 | Discriminator Loss: 1.3957688808441162\n","Epoch: 153 | Generator Loss: 0.7057466506958008 | Discriminator Loss: 1.4176325798034668\n","Epoch: 154 | Generator Loss: 0.7580766081809998 | Discriminator Loss: 1.3201640844345093\n","Epoch: 155 | Generator Loss: 0.7169897556304932 | Discriminator Loss: 1.3825716972351074\n","Epoch: 156 | Generator Loss: 0.7538106441497803 | Discriminator Loss: 1.3521482944488525\n","Epoch: 157 | Generator Loss: 0.7517910003662109 | Discriminator Loss: 1.386908769607544\n","Epoch: 158 | Generator Loss: 0.750093936920166 | Discriminator Loss: 1.3275525569915771\n","Epoch: 159 | Generator Loss: 0.7716002464294434 | Discriminator Loss: 1.3304386138916016\n","Epoch: 160 | Generator Loss: 0.7590633630752563 | Discriminator Loss: 1.409781575202942\n","Epoch: 161 | Generator Loss: 0.7744957208633423 | Discriminator Loss: 1.456437110900879\n","Epoch: 162 | Generator Loss: 0.7308807373046875 | Discriminator Loss: 1.4025719165802002\n","Epoch: 163 | Generator Loss: 0.7264623045921326 | Discriminator Loss: 1.2984944581985474\n","Epoch: 164 | Generator Loss: 0.7153081893920898 | Discriminator Loss: 1.3093308210372925\n","Epoch: 165 | Generator Loss: 0.7674407958984375 | Discriminator Loss: 1.3074302673339844\n","Epoch: 166 | Generator Loss: 0.7129023671150208 | Discriminator Loss: 1.48652982711792\n","Epoch: 167 | Generator Loss: 0.750340461730957 | Discriminator Loss: 1.2311556339263916\n","Epoch: 168 | Generator Loss: 0.7320761680603027 | Discriminator Loss: 1.3741512298583984\n","Epoch: 169 | Generator Loss: 0.718797504901886 | Discriminator Loss: 1.3962405920028687\n","Epoch: 170 | Generator Loss: 0.6935679912567139 | Discriminator Loss: 1.322946548461914\n","Epoch: 171 | Generator Loss: 0.7609632015228271 | Discriminator Loss: 1.3574814796447754\n","Epoch: 172 | Generator Loss: 0.7128065824508667 | Discriminator Loss: 1.4004508256912231\n","Epoch: 173 | Generator Loss: 0.7343804836273193 | Discriminator Loss: 1.3875596523284912\n","Epoch: 174 | Generator Loss: 0.7322008609771729 | Discriminator Loss: 1.3780920505523682\n","Epoch: 175 | Generator Loss: 0.7064946889877319 | Discriminator Loss: 1.3302083015441895\n","Epoch: 176 | Generator Loss: 0.7570996284484863 | Discriminator Loss: 1.4023785591125488\n","Epoch: 177 | Generator Loss: 0.7162643074989319 | Discriminator Loss: 1.3438981771469116\n","Epoch: 178 | Generator Loss: 0.7608669996261597 | Discriminator Loss: 1.4239219427108765\n","Epoch: 179 | Generator Loss: 0.7535130977630615 | Discriminator Loss: 1.4303306341171265\n","Epoch: 180 | Generator Loss: 0.7560281157493591 | Discriminator Loss: 1.3455359935760498\n","Epoch: 181 | Generator Loss: 0.7323383688926697 | Discriminator Loss: 1.345433235168457\n","Epoch: 182 | Generator Loss: 0.6965498924255371 | Discriminator Loss: 1.3949707746505737\n","Epoch: 183 | Generator Loss: 0.6871615648269653 | Discriminator Loss: 1.345578670501709\n","Epoch: 184 | Generator Loss: 0.7495170831680298 | Discriminator Loss: 1.3530948162078857\n","Epoch: 185 | Generator Loss: 0.7190583348274231 | Discriminator Loss: 1.3206017017364502\n","Epoch: 186 | Generator Loss: 0.753827691078186 | Discriminator Loss: 1.311510443687439\n","Epoch: 187 | Generator Loss: 0.7300017476081848 | Discriminator Loss: 1.3746337890625\n","Epoch: 188 | Generator Loss: 0.7532600164413452 | Discriminator Loss: 1.3888531923294067\n","Epoch: 189 | Generator Loss: 0.746441125869751 | Discriminator Loss: 1.3731368780136108\n","Epoch: 190 | Generator Loss: 0.7354335784912109 | Discriminator Loss: 1.3482513427734375\n","Epoch: 191 | Generator Loss: 0.7213670015335083 | Discriminator Loss: 1.3051769733428955\n","Epoch: 192 | Generator Loss: 0.6823751926422119 | Discriminator Loss: 1.343056082725525\n","Epoch: 193 | Generator Loss: 0.7410554885864258 | Discriminator Loss: 1.4069404602050781\n","Epoch: 194 | Generator Loss: 0.7196308970451355 | Discriminator Loss: 1.4054079055786133\n","Epoch: 195 | Generator Loss: 0.7208279967308044 | Discriminator Loss: 1.425706386566162\n","Epoch: 196 | Generator Loss: 0.7454820871353149 | Discriminator Loss: 1.321063756942749\n","Epoch: 197 | Generator Loss: 0.7127860188484192 | Discriminator Loss: 1.3110295534133911\n","Epoch: 198 | Generator Loss: 0.744081437587738 | Discriminator Loss: 1.3269007205963135\n","Epoch: 199 | Generator Loss: 0.7421903610229492 | Discriminator Loss: 1.3715122938156128\n","Epoch: 200 | Generator Loss: 0.7546335458755493 | Discriminator Loss: 1.362671136856079\n","Epoch: 201 | Generator Loss: 0.7354521751403809 | Discriminator Loss: 1.4032948017120361\n","Epoch: 202 | Generator Loss: 0.7369072437286377 | Discriminator Loss: 1.3836098909378052\n","Epoch: 203 | Generator Loss: 0.7171266078948975 | Discriminator Loss: 1.3740339279174805\n","Epoch: 204 | Generator Loss: 0.7334031462669373 | Discriminator Loss: 1.4296081066131592\n","Epoch: 205 | Generator Loss: 0.6842375993728638 | Discriminator Loss: 1.3066390752792358\n","Epoch: 206 | Generator Loss: 0.706731915473938 | Discriminator Loss: 1.3592352867126465\n","Epoch: 207 | Generator Loss: 0.705460786819458 | Discriminator Loss: 1.3724520206451416\n","Epoch: 208 | Generator Loss: 0.70140540599823 | Discriminator Loss: 1.3958340883255005\n","Epoch: 209 | Generator Loss: 0.7469640970230103 | Discriminator Loss: 1.3195395469665527\n","Epoch: 210 | Generator Loss: 0.6966155767440796 | Discriminator Loss: 1.3587855100631714\n","Epoch: 211 | Generator Loss: 0.7727052569389343 | Discriminator Loss: 1.3311562538146973\n","Epoch: 212 | Generator Loss: 0.740355372428894 | Discriminator Loss: 1.2753607034683228\n","Epoch: 213 | Generator Loss: 0.7359300851821899 | Discriminator Loss: 1.3646568059921265\n","Epoch: 214 | Generator Loss: 0.7457766532897949 | Discriminator Loss: 1.3240691423416138\n","Epoch: 215 | Generator Loss: 0.713675320148468 | Discriminator Loss: 1.3962947130203247\n","Epoch: 216 | Generator Loss: 0.7154982686042786 | Discriminator Loss: 1.380605936050415\n","Epoch: 217 | Generator Loss: 0.7068798542022705 | Discriminator Loss: 1.3300411701202393\n","Epoch: 218 | Generator Loss: 0.7319827079772949 | Discriminator Loss: 1.3671009540557861\n","Epoch: 219 | Generator Loss: 0.7295007705688477 | Discriminator Loss: 1.3327889442443848\n","Epoch: 220 | Generator Loss: 0.7868266701698303 | Discriminator Loss: 1.3882319927215576\n","Epoch: 221 | Generator Loss: 0.7349876165390015 | Discriminator Loss: 1.3690749406814575\n","Epoch: 222 | Generator Loss: 0.7647619247436523 | Discriminator Loss: 1.3459171056747437\n","Epoch: 223 | Generator Loss: 0.7369830012321472 | Discriminator Loss: 1.3700428009033203\n","Epoch: 224 | Generator Loss: 0.7467781901359558 | Discriminator Loss: 1.3488013744354248\n","Epoch: 225 | Generator Loss: 0.7331715226173401 | Discriminator Loss: 1.3212788105010986\n","Epoch: 226 | Generator Loss: 0.7319881319999695 | Discriminator Loss: 1.3730798959732056\n","Epoch: 227 | Generator Loss: 0.7278169393539429 | Discriminator Loss: 1.327384114265442\n","Epoch: 228 | Generator Loss: 0.7253900766372681 | Discriminator Loss: 1.3180744647979736\n","Epoch: 229 | Generator Loss: 0.734784722328186 | Discriminator Loss: 1.2949494123458862\n","Epoch: 230 | Generator Loss: 0.7588072419166565 | Discriminator Loss: 1.2830634117126465\n","Epoch: 231 | Generator Loss: 0.7560389637947083 | Discriminator Loss: 1.3930997848510742\n","Epoch: 232 | Generator Loss: 0.7388198375701904 | Discriminator Loss: 1.3134995698928833\n","Epoch: 233 | Generator Loss: 0.7334866523742676 | Discriminator Loss: 1.3676600456237793\n","Epoch: 234 | Generator Loss: 0.6841102838516235 | Discriminator Loss: 1.3946950435638428\n","Epoch: 235 | Generator Loss: 0.7045292854309082 | Discriminator Loss: 1.307835340499878\n","Epoch: 236 | Generator Loss: 0.7167718410491943 | Discriminator Loss: 1.3664638996124268\n","Epoch: 237 | Generator Loss: 0.7397845983505249 | Discriminator Loss: 1.3664319515228271\n","Epoch: 238 | Generator Loss: 0.6809781789779663 | Discriminator Loss: 1.3279789686203003\n","Epoch: 239 | Generator Loss: 0.7115731835365295 | Discriminator Loss: 1.4101643562316895\n","Epoch: 240 | Generator Loss: 0.7211881875991821 | Discriminator Loss: 1.276661992073059\n","Epoch: 241 | Generator Loss: 0.729121744632721 | Discriminator Loss: 1.3643964529037476\n","Epoch: 242 | Generator Loss: 0.7115077376365662 | Discriminator Loss: 1.374974012374878\n","Epoch: 243 | Generator Loss: 0.7520866394042969 | Discriminator Loss: 1.2893136739730835\n","Epoch: 244 | Generator Loss: 0.7397891283035278 | Discriminator Loss: 1.4087984561920166\n","Epoch: 245 | Generator Loss: 0.7304235696792603 | Discriminator Loss: 1.3528964519500732\n","Epoch: 246 | Generator Loss: 0.7468969225883484 | Discriminator Loss: 1.3100236654281616\n","Epoch: 247 | Generator Loss: 0.7184668779373169 | Discriminator Loss: 1.3636775016784668\n","Epoch: 248 | Generator Loss: 0.7384737730026245 | Discriminator Loss: 1.375406265258789\n","Epoch: 249 | Generator Loss: 0.7563377618789673 | Discriminator Loss: 1.2865123748779297\n","Epoch: 250 | Generator Loss: 0.7981374263763428 | Discriminator Loss: 1.3010588884353638\n","Epoch: 251 | Generator Loss: 0.7098236680030823 | Discriminator Loss: 1.3266963958740234\n","Epoch: 252 | Generator Loss: 0.7367944717407227 | Discriminator Loss: 1.3842276334762573\n","Epoch: 253 | Generator Loss: 0.7286053895950317 | Discriminator Loss: 1.2731114625930786\n","Epoch: 254 | Generator Loss: 0.720527172088623 | Discriminator Loss: 1.3309630155563354\n","Epoch: 255 | Generator Loss: 0.71687251329422 | Discriminator Loss: 1.3597021102905273\n","Epoch: 256 | Generator Loss: 0.7391194701194763 | Discriminator Loss: 1.4235949516296387\n","Epoch: 257 | Generator Loss: 0.7393251657485962 | Discriminator Loss: 1.3034164905548096\n","Epoch: 258 | Generator Loss: 0.7163037061691284 | Discriminator Loss: 1.330085039138794\n","Epoch: 259 | Generator Loss: 0.718152642250061 | Discriminator Loss: 1.3960334062576294\n","Epoch: 260 | Generator Loss: 0.7763527631759644 | Discriminator Loss: 1.3658661842346191\n","Epoch: 261 | Generator Loss: 0.7628012895584106 | Discriminator Loss: 1.4147582054138184\n","Epoch: 262 | Generator Loss: 0.7334038615226746 | Discriminator Loss: 1.3780395984649658\n","Epoch: 263 | Generator Loss: 0.7172694206237793 | Discriminator Loss: 1.3036472797393799\n","Epoch: 264 | Generator Loss: 0.7399400472640991 | Discriminator Loss: 1.2832555770874023\n","Epoch: 265 | Generator Loss: 0.7621928453445435 | Discriminator Loss: 1.321205973625183\n","Epoch: 266 | Generator Loss: 0.7241981029510498 | Discriminator Loss: 1.2763177156448364\n","Epoch: 267 | Generator Loss: 0.750309407711029 | Discriminator Loss: 1.336066722869873\n","Epoch: 268 | Generator Loss: 0.7784325480461121 | Discriminator Loss: 1.3768346309661865\n","Epoch: 269 | Generator Loss: 0.7389109134674072 | Discriminator Loss: 1.300105333328247\n","Epoch: 270 | Generator Loss: 0.7059627771377563 | Discriminator Loss: 1.4460777044296265\n","Epoch: 271 | Generator Loss: 0.7402931451797485 | Discriminator Loss: 1.3448154926300049\n","Epoch: 272 | Generator Loss: 0.7393206357955933 | Discriminator Loss: 1.2929331064224243\n","Epoch: 273 | Generator Loss: 0.7136213779449463 | Discriminator Loss: 1.3245162963867188\n","Epoch: 274 | Generator Loss: 0.7214492559432983 | Discriminator Loss: 1.3763798475265503\n","Epoch: 275 | Generator Loss: 0.7701206207275391 | Discriminator Loss: 1.3569762706756592\n","Epoch: 276 | Generator Loss: 0.740409791469574 | Discriminator Loss: 1.3743834495544434\n","Epoch: 277 | Generator Loss: 0.7151779532432556 | Discriminator Loss: 1.3282514810562134\n","Epoch: 278 | Generator Loss: 0.7015622854232788 | Discriminator Loss: 1.3689897060394287\n","Epoch: 279 | Generator Loss: 0.7126595973968506 | Discriminator Loss: 1.363438367843628\n","Epoch: 280 | Generator Loss: 0.7393746972084045 | Discriminator Loss: 1.2595508098602295\n","Epoch: 281 | Generator Loss: 0.7457715272903442 | Discriminator Loss: 1.3078194856643677\n","Epoch: 282 | Generator Loss: 0.7395452857017517 | Discriminator Loss: 1.340437412261963\n","Epoch: 283 | Generator Loss: 0.7065651416778564 | Discriminator Loss: 1.3945074081420898\n","Epoch: 284 | Generator Loss: 0.7415832877159119 | Discriminator Loss: 1.3218002319335938\n","Epoch: 285 | Generator Loss: 0.7101197242736816 | Discriminator Loss: 1.2917544841766357\n","Epoch: 286 | Generator Loss: 0.7334997653961182 | Discriminator Loss: 1.3629536628723145\n","Epoch: 287 | Generator Loss: 0.7278395295143127 | Discriminator Loss: 1.4239435195922852\n","Epoch: 288 | Generator Loss: 0.7098889946937561 | Discriminator Loss: 1.2898244857788086\n","Epoch: 289 | Generator Loss: 0.7204914093017578 | Discriminator Loss: 1.3301981687545776\n","Epoch: 290 | Generator Loss: 0.7317790389060974 | Discriminator Loss: 1.2457661628723145\n","Epoch: 291 | Generator Loss: 0.743811309337616 | Discriminator Loss: 1.3567103147506714\n","Epoch: 292 | Generator Loss: 0.744152307510376 | Discriminator Loss: 1.3290005922317505\n","Epoch: 293 | Generator Loss: 0.7364697456359863 | Discriminator Loss: 1.3347662687301636\n","Epoch: 294 | Generator Loss: 0.7007686495780945 | Discriminator Loss: 1.3540880680084229\n","Epoch: 295 | Generator Loss: 0.7497178912162781 | Discriminator Loss: 1.3205753564834595\n","Epoch: 296 | Generator Loss: 0.7427396774291992 | Discriminator Loss: 1.292857050895691\n","Epoch: 297 | Generator Loss: 0.7283250689506531 | Discriminator Loss: 1.297636866569519\n","Epoch: 298 | Generator Loss: 0.7387495636940002 | Discriminator Loss: 1.3559272289276123\n","Epoch: 299 | Generator Loss: 0.7289496660232544 | Discriminator Loss: 1.3489446640014648\n","Epoch: 300 | Generator Loss: 0.7542568445205688 | Discriminator Loss: 1.3427520990371704\n","Epoch: 301 | Generator Loss: 0.720462441444397 | Discriminator Loss: 1.4124877452850342\n","Epoch: 302 | Generator Loss: 0.7030966877937317 | Discriminator Loss: 1.3491177558898926\n","Epoch: 303 | Generator Loss: 0.7275047302246094 | Discriminator Loss: 1.3262691497802734\n","Epoch: 304 | Generator Loss: 0.7414110898971558 | Discriminator Loss: 1.3748362064361572\n","Epoch: 305 | Generator Loss: 0.7851054668426514 | Discriminator Loss: 1.368332862854004\n","Epoch: 306 | Generator Loss: 0.7054358124732971 | Discriminator Loss: 1.2807526588439941\n","Epoch: 307 | Generator Loss: 0.7963016033172607 | Discriminator Loss: 1.2683782577514648\n","Epoch: 308 | Generator Loss: 0.735942542552948 | Discriminator Loss: 1.3578674793243408\n","Epoch: 309 | Generator Loss: 0.7261351346969604 | Discriminator Loss: 1.3452692031860352\n","Epoch: 310 | Generator Loss: 0.7080242037773132 | Discriminator Loss: 1.3581767082214355\n","Epoch: 311 | Generator Loss: 0.7239577770233154 | Discriminator Loss: 1.3075015544891357\n","Epoch: 312 | Generator Loss: 0.7235152721405029 | Discriminator Loss: 1.3442296981811523\n","Epoch: 313 | Generator Loss: 0.692310094833374 | Discriminator Loss: 1.3637909889221191\n","Epoch: 314 | Generator Loss: 0.7432045340538025 | Discriminator Loss: 1.3330588340759277\n","Epoch: 315 | Generator Loss: 0.6921200752258301 | Discriminator Loss: 1.3440378904342651\n","Epoch: 316 | Generator Loss: 0.6729987859725952 | Discriminator Loss: 1.3451650142669678\n","Epoch: 317 | Generator Loss: 0.7570451498031616 | Discriminator Loss: 1.2787154912948608\n","Epoch: 318 | Generator Loss: 0.6995783448219299 | Discriminator Loss: 1.3926279544830322\n","Epoch: 319 | Generator Loss: 0.7494659423828125 | Discriminator Loss: 1.354999303817749\n","Epoch: 320 | Generator Loss: 0.7311702966690063 | Discriminator Loss: 1.319922924041748\n","Epoch: 321 | Generator Loss: 0.7196068167686462 | Discriminator Loss: 1.3047064542770386\n","Epoch: 322 | Generator Loss: 0.7440776824951172 | Discriminator Loss: 1.327319860458374\n","Epoch: 323 | Generator Loss: 0.7171573638916016 | Discriminator Loss: 1.419498324394226\n","Epoch: 324 | Generator Loss: 0.7429248690605164 | Discriminator Loss: 1.3562309741973877\n","Epoch: 325 | Generator Loss: 0.7526025176048279 | Discriminator Loss: 1.3684489727020264\n","Epoch: 326 | Generator Loss: 0.729995608329773 | Discriminator Loss: 1.321537733078003\n","Epoch: 327 | Generator Loss: 0.7091572880744934 | Discriminator Loss: 1.3032652139663696\n","Epoch: 328 | Generator Loss: 0.7514292001724243 | Discriminator Loss: 1.3365249633789062\n","Epoch: 329 | Generator Loss: 0.724025309085846 | Discriminator Loss: 1.293736219406128\n","Epoch: 330 | Generator Loss: 0.7159746289253235 | Discriminator Loss: 1.3289031982421875\n","Epoch: 331 | Generator Loss: 0.7251521348953247 | Discriminator Loss: 1.319626808166504\n","Epoch: 332 | Generator Loss: 0.721039891242981 | Discriminator Loss: 1.3364135026931763\n","Epoch: 333 | Generator Loss: 0.7199113368988037 | Discriminator Loss: 1.3769505023956299\n","Epoch: 334 | Generator Loss: 0.723825216293335 | Discriminator Loss: 1.3042023181915283\n","Epoch: 335 | Generator Loss: 0.76024329662323 | Discriminator Loss: 1.4133620262145996\n","Epoch: 336 | Generator Loss: 0.7411207556724548 | Discriminator Loss: 1.359036922454834\n","Epoch: 337 | Generator Loss: 0.734783947467804 | Discriminator Loss: 1.2927474975585938\n","Epoch: 338 | Generator Loss: 0.7395825982093811 | Discriminator Loss: 1.4013359546661377\n","Epoch: 339 | Generator Loss: 0.7584337592124939 | Discriminator Loss: 1.32560396194458\n","Epoch: 340 | Generator Loss: 0.726233720779419 | Discriminator Loss: 1.3237488269805908\n","Epoch: 341 | Generator Loss: 0.7368887662887573 | Discriminator Loss: 1.3359885215759277\n","Epoch: 342 | Generator Loss: 0.7119020223617554 | Discriminator Loss: 1.2936108112335205\n","Epoch: 343 | Generator Loss: 0.7089862823486328 | Discriminator Loss: 1.3664000034332275\n","Epoch: 344 | Generator Loss: 0.768850564956665 | Discriminator Loss: 1.371138572692871\n","Epoch: 345 | Generator Loss: 0.7895960807800293 | Discriminator Loss: 1.314483880996704\n","Epoch: 346 | Generator Loss: 0.7451124787330627 | Discriminator Loss: 1.345117449760437\n","Epoch: 347 | Generator Loss: 0.7677053809165955 | Discriminator Loss: 1.385708212852478\n","Epoch: 348 | Generator Loss: 0.7032164335250854 | Discriminator Loss: 1.322434425354004\n","Epoch: 349 | Generator Loss: 0.7623587846755981 | Discriminator Loss: 1.3694534301757812\n","Epoch: 350 | Generator Loss: 0.7454273104667664 | Discriminator Loss: 1.2853221893310547\n","Epoch: 351 | Generator Loss: 0.7508030533790588 | Discriminator Loss: 1.3506317138671875\n","Epoch: 352 | Generator Loss: 0.7526886463165283 | Discriminator Loss: 1.3672399520874023\n","Epoch: 353 | Generator Loss: 0.727808952331543 | Discriminator Loss: 1.2826358079910278\n","Epoch: 354 | Generator Loss: 0.7260450124740601 | Discriminator Loss: 1.348884105682373\n","Epoch: 355 | Generator Loss: 0.7422462701797485 | Discriminator Loss: 1.329267978668213\n","Epoch: 356 | Generator Loss: 0.7420156002044678 | Discriminator Loss: 1.290831446647644\n","Epoch: 357 | Generator Loss: 0.733189582824707 | Discriminator Loss: 1.3993098735809326\n","Epoch: 358 | Generator Loss: 0.7271270751953125 | Discriminator Loss: 1.3186180591583252\n","Epoch: 359 | Generator Loss: 0.7272520065307617 | Discriminator Loss: 1.366720199584961\n","Epoch: 360 | Generator Loss: 0.7464578747749329 | Discriminator Loss: 1.272843360900879\n","Epoch: 361 | Generator Loss: 0.7535288333892822 | Discriminator Loss: 1.3087022304534912\n","Epoch: 362 | Generator Loss: 0.7972617149353027 | Discriminator Loss: 1.3416708707809448\n","Epoch: 363 | Generator Loss: 0.7446495294570923 | Discriminator Loss: 1.3032805919647217\n","Epoch: 364 | Generator Loss: 0.7624252438545227 | Discriminator Loss: 1.3915305137634277\n","Epoch: 365 | Generator Loss: 0.766782283782959 | Discriminator Loss: 1.3393867015838623\n","Epoch: 366 | Generator Loss: 0.7207660675048828 | Discriminator Loss: 1.3625271320343018\n","Epoch: 367 | Generator Loss: 0.7416789531707764 | Discriminator Loss: 1.2750346660614014\n","Epoch: 368 | Generator Loss: 0.7534809112548828 | Discriminator Loss: 1.2850559949874878\n","Epoch: 369 | Generator Loss: 0.7341340780258179 | Discriminator Loss: 1.3355891704559326\n","Epoch: 370 | Generator Loss: 0.7496364116668701 | Discriminator Loss: 1.2569103240966797\n","Epoch: 371 | Generator Loss: 0.7696434259414673 | Discriminator Loss: 1.3319162130355835\n","Epoch: 372 | Generator Loss: 0.7109547257423401 | Discriminator Loss: 1.3557394742965698\n","Epoch: 373 | Generator Loss: 0.7461179494857788 | Discriminator Loss: 1.2791168689727783\n","Epoch: 374 | Generator Loss: 0.7247440814971924 | Discriminator Loss: 1.3168940544128418\n","Epoch: 375 | Generator Loss: 0.7415260076522827 | Discriminator Loss: 1.37067711353302\n","Epoch: 376 | Generator Loss: 0.7331832647323608 | Discriminator Loss: 1.3540539741516113\n","Epoch: 377 | Generator Loss: 0.768811821937561 | Discriminator Loss: 1.3471730947494507\n","Epoch: 378 | Generator Loss: 0.7631621360778809 | Discriminator Loss: 1.338111162185669\n","Epoch: 379 | Generator Loss: 0.7394593358039856 | Discriminator Loss: 1.2711009979248047\n","Epoch: 380 | Generator Loss: 0.7991279363632202 | Discriminator Loss: 1.2323501110076904\n","Epoch: 381 | Generator Loss: 0.7312643527984619 | Discriminator Loss: 1.3405814170837402\n","Epoch: 382 | Generator Loss: 0.7477566599845886 | Discriminator Loss: 1.3425321578979492\n","Epoch: 383 | Generator Loss: 0.7573136687278748 | Discriminator Loss: 1.3494863510131836\n","Epoch: 384 | Generator Loss: 0.7619742155075073 | Discriminator Loss: 1.3311517238616943\n","Epoch: 385 | Generator Loss: 0.7642155885696411 | Discriminator Loss: 1.2565251588821411\n","Epoch: 386 | Generator Loss: 0.7860416173934937 | Discriminator Loss: 1.2949645519256592\n","Epoch: 387 | Generator Loss: 0.7165156602859497 | Discriminator Loss: 1.3596380949020386\n","Epoch: 388 | Generator Loss: 0.722598135471344 | Discriminator Loss: 1.2386970520019531\n","Epoch: 389 | Generator Loss: 0.7335494160652161 | Discriminator Loss: 1.306818962097168\n","Epoch: 390 | Generator Loss: 0.7276899218559265 | Discriminator Loss: 1.294607162475586\n","Epoch: 391 | Generator Loss: 0.6992179155349731 | Discriminator Loss: 1.281001329421997\n","Epoch: 392 | Generator Loss: 0.7110190391540527 | Discriminator Loss: 1.2869113683700562\n","Epoch: 393 | Generator Loss: 0.7177931070327759 | Discriminator Loss: 1.3051609992980957\n","Epoch: 394 | Generator Loss: 0.7422983646392822 | Discriminator Loss: 1.3568921089172363\n","Epoch: 395 | Generator Loss: 0.724603533744812 | Discriminator Loss: 1.3717143535614014\n","Epoch: 396 | Generator Loss: 0.7500335574150085 | Discriminator Loss: 1.300001621246338\n","Epoch: 397 | Generator Loss: 0.7686082124710083 | Discriminator Loss: 1.299971342086792\n","Epoch: 398 | Generator Loss: 0.7340255975723267 | Discriminator Loss: 1.3785600662231445\n","Epoch: 399 | Generator Loss: 0.7469484806060791 | Discriminator Loss: 1.3187118768692017\n","Epoch: 400 | Generator Loss: 0.7606212496757507 | Discriminator Loss: 1.346639633178711\n","Epoch: 401 | Generator Loss: 0.7175881862640381 | Discriminator Loss: 1.3225817680358887\n","Epoch: 402 | Generator Loss: 0.7040086984634399 | Discriminator Loss: 1.330615520477295\n","Epoch: 403 | Generator Loss: 0.7368404269218445 | Discriminator Loss: 1.3656249046325684\n","Epoch: 404 | Generator Loss: 0.7322986721992493 | Discriminator Loss: 1.3945176601409912\n","Epoch: 405 | Generator Loss: 0.7008973360061646 | Discriminator Loss: 1.3593367338180542\n","Epoch: 406 | Generator Loss: 0.7524617910385132 | Discriminator Loss: 1.350311279296875\n","Epoch: 407 | Generator Loss: 0.7238811254501343 | Discriminator Loss: 1.3521976470947266\n","Epoch: 408 | Generator Loss: 0.708349883556366 | Discriminator Loss: 1.378190040588379\n","Epoch: 409 | Generator Loss: 0.7673887014389038 | Discriminator Loss: 1.33168363571167\n","Epoch: 410 | Generator Loss: 0.7363873720169067 | Discriminator Loss: 1.306981086730957\n","Epoch: 411 | Generator Loss: 0.707822322845459 | Discriminator Loss: 1.3472955226898193\n","Epoch: 412 | Generator Loss: 0.780480146408081 | Discriminator Loss: 1.3435181379318237\n","Epoch: 413 | Generator Loss: 0.7317303419113159 | Discriminator Loss: 1.3161686658859253\n","Epoch: 414 | Generator Loss: 0.7402409315109253 | Discriminator Loss: 1.3163803815841675\n","Epoch: 415 | Generator Loss: 0.6978023052215576 | Discriminator Loss: 1.3420217037200928\n","Epoch: 416 | Generator Loss: 0.737948477268219 | Discriminator Loss: 1.2192987203598022\n","Epoch: 417 | Generator Loss: 0.7226581573486328 | Discriminator Loss: 1.302598476409912\n","Epoch: 418 | Generator Loss: 0.7060609459877014 | Discriminator Loss: 1.3677623271942139\n","Epoch: 419 | Generator Loss: 0.7304397225379944 | Discriminator Loss: 1.3806647062301636\n","Epoch: 420 | Generator Loss: 0.7271041870117188 | Discriminator Loss: 1.333287000656128\n","Epoch: 421 | Generator Loss: 0.7575727701187134 | Discriminator Loss: 1.2489312887191772\n","Epoch: 422 | Generator Loss: 0.710067868232727 | Discriminator Loss: 1.387244701385498\n","Epoch: 423 | Generator Loss: 0.7217462062835693 | Discriminator Loss: 1.2295081615447998\n","Epoch: 424 | Generator Loss: 0.7453150749206543 | Discriminator Loss: 1.3225674629211426\n","Epoch: 425 | Generator Loss: 0.713768720626831 | Discriminator Loss: 1.2663145065307617\n","Epoch: 426 | Generator Loss: 0.7694761753082275 | Discriminator Loss: 1.33469557762146\n","Epoch: 427 | Generator Loss: 0.7158310413360596 | Discriminator Loss: 1.280626654624939\n","Epoch: 428 | Generator Loss: 0.7245558500289917 | Discriminator Loss: 1.3349976539611816\n","Epoch: 429 | Generator Loss: 0.7092989087104797 | Discriminator Loss: 1.34927499294281\n","Epoch: 430 | Generator Loss: 0.6954606771469116 | Discriminator Loss: 1.2896568775177002\n","Epoch: 431 | Generator Loss: 0.7049229145050049 | Discriminator Loss: 1.2215821743011475\n","Epoch: 432 | Generator Loss: 0.7241395711898804 | Discriminator Loss: 1.374711036682129\n","Epoch: 433 | Generator Loss: 0.7065240740776062 | Discriminator Loss: 1.331214427947998\n","Epoch: 434 | Generator Loss: 0.7304587364196777 | Discriminator Loss: 1.2210503816604614\n","Epoch: 435 | Generator Loss: 0.741950511932373 | Discriminator Loss: 1.3447299003601074\n","Epoch: 436 | Generator Loss: 0.7480220794677734 | Discriminator Loss: 1.3355506658554077\n","Epoch: 437 | Generator Loss: 0.7528182864189148 | Discriminator Loss: 1.2769954204559326\n","Epoch: 438 | Generator Loss: 0.7947556972503662 | Discriminator Loss: 1.405994176864624\n","Epoch: 439 | Generator Loss: 0.7310420274734497 | Discriminator Loss: 1.322466254234314\n","Epoch: 440 | Generator Loss: 0.7529897689819336 | Discriminator Loss: 1.3293572664260864\n","Epoch: 441 | Generator Loss: 0.7287733554840088 | Discriminator Loss: 1.3149036169052124\n","Epoch: 442 | Generator Loss: 0.76729816198349 | Discriminator Loss: 1.34511137008667\n","Epoch: 443 | Generator Loss: 0.7693403959274292 | Discriminator Loss: 1.218553066253662\n","Epoch: 444 | Generator Loss: 0.7148420810699463 | Discriminator Loss: 1.334639072418213\n","Epoch: 445 | Generator Loss: 0.8077986240386963 | Discriminator Loss: 1.287045955657959\n","Epoch: 446 | Generator Loss: 0.7611717581748962 | Discriminator Loss: 1.2443187236785889\n","Epoch: 447 | Generator Loss: 0.7288533449172974 | Discriminator Loss: 1.3208543062210083\n","Epoch: 448 | Generator Loss: 0.6940567493438721 | Discriminator Loss: 1.344525933265686\n","Epoch: 449 | Generator Loss: 0.6976445317268372 | Discriminator Loss: 1.2790629863739014\n","Epoch: 450 | Generator Loss: 0.6776841878890991 | Discriminator Loss: 1.2717911005020142\n","Epoch: 451 | Generator Loss: 0.7393074035644531 | Discriminator Loss: 1.3625454902648926\n","Epoch: 452 | Generator Loss: 0.7075002193450928 | Discriminator Loss: 1.3975021839141846\n","Epoch: 453 | Generator Loss: 0.7019040584564209 | Discriminator Loss: 1.277815818786621\n","Epoch: 454 | Generator Loss: 0.7393358945846558 | Discriminator Loss: 1.265626311302185\n","Epoch: 455 | Generator Loss: 0.7585930824279785 | Discriminator Loss: 1.2961543798446655\n","Epoch: 456 | Generator Loss: 0.7728766202926636 | Discriminator Loss: 1.348573088645935\n","Epoch: 457 | Generator Loss: 0.6815714836120605 | Discriminator Loss: 1.3308525085449219\n","Epoch: 458 | Generator Loss: 0.7405744791030884 | Discriminator Loss: 1.2634752988815308\n","Epoch: 459 | Generator Loss: 0.759460985660553 | Discriminator Loss: 1.2400383949279785\n","Epoch: 460 | Generator Loss: 0.7275646924972534 | Discriminator Loss: 1.2500674724578857\n","Epoch: 461 | Generator Loss: 0.714824914932251 | Discriminator Loss: 1.3326425552368164\n","Epoch: 462 | Generator Loss: 0.7367074489593506 | Discriminator Loss: 1.300079584121704\n","Epoch: 463 | Generator Loss: 0.7560893297195435 | Discriminator Loss: 1.3367037773132324\n","Epoch: 464 | Generator Loss: 0.7321662902832031 | Discriminator Loss: 1.3096678256988525\n","Epoch: 465 | Generator Loss: 0.7380883693695068 | Discriminator Loss: 1.3325247764587402\n","Epoch: 466 | Generator Loss: 0.7428979873657227 | Discriminator Loss: 1.287833333015442\n","Epoch: 467 | Generator Loss: 0.7541481852531433 | Discriminator Loss: 1.3483667373657227\n","Epoch: 468 | Generator Loss: 0.7352551221847534 | Discriminator Loss: 1.273296594619751\n","Epoch: 469 | Generator Loss: 0.7127890586853027 | Discriminator Loss: 1.360414743423462\n","Epoch: 470 | Generator Loss: 0.7402077317237854 | Discriminator Loss: 1.3073618412017822\n","Epoch: 471 | Generator Loss: 0.7329893112182617 | Discriminator Loss: 1.250455617904663\n","Epoch: 472 | Generator Loss: 0.7250887155532837 | Discriminator Loss: 1.2719295024871826\n","Epoch: 473 | Generator Loss: 0.6967912912368774 | Discriminator Loss: 1.3831831216812134\n","Epoch: 474 | Generator Loss: 0.74570232629776 | Discriminator Loss: 1.363328218460083\n","Epoch: 475 | Generator Loss: 0.7274593114852905 | Discriminator Loss: 1.310774564743042\n","Epoch: 476 | Generator Loss: 0.7277384996414185 | Discriminator Loss: 1.3434462547302246\n","Epoch: 477 | Generator Loss: 0.7408233284950256 | Discriminator Loss: 1.2777830362319946\n","Epoch: 478 | Generator Loss: 0.753273606300354 | Discriminator Loss: 1.3122187852859497\n","Epoch: 479 | Generator Loss: 0.7621172666549683 | Discriminator Loss: 1.3292165994644165\n","Epoch: 480 | Generator Loss: 0.7305785417556763 | Discriminator Loss: 1.3299908638000488\n","Epoch: 481 | Generator Loss: 0.7290674448013306 | Discriminator Loss: 1.3254458904266357\n","Epoch: 482 | Generator Loss: 0.7379624843597412 | Discriminator Loss: 1.344405174255371\n","Epoch: 483 | Generator Loss: 0.7659110426902771 | Discriminator Loss: 1.3304967880249023\n","Epoch: 484 | Generator Loss: 0.7580223679542542 | Discriminator Loss: 1.2082254886627197\n","Epoch: 485 | Generator Loss: 0.7169070243835449 | Discriminator Loss: 1.3169758319854736\n","Epoch: 486 | Generator Loss: 0.7167942523956299 | Discriminator Loss: 1.3272268772125244\n","Epoch: 487 | Generator Loss: 0.7797259092330933 | Discriminator Loss: 1.3701999187469482\n","Epoch: 488 | Generator Loss: 0.7359368801116943 | Discriminator Loss: 1.3273630142211914\n","Epoch: 489 | Generator Loss: 0.7481156587600708 | Discriminator Loss: 1.2864818572998047\n","Epoch: 490 | Generator Loss: 0.7371956706047058 | Discriminator Loss: 1.37176513671875\n","Epoch: 491 | Generator Loss: 0.6918123960494995 | Discriminator Loss: 1.3175275325775146\n","Epoch: 492 | Generator Loss: 0.7665114402770996 | Discriminator Loss: 1.2930068969726562\n","Epoch: 493 | Generator Loss: 0.7741705775260925 | Discriminator Loss: 1.3267645835876465\n","Epoch: 494 | Generator Loss: 0.7516549825668335 | Discriminator Loss: 1.2607524394989014\n","Epoch: 495 | Generator Loss: 0.774838924407959 | Discriminator Loss: 1.258769154548645\n","Epoch: 496 | Generator Loss: 0.7155494689941406 | Discriminator Loss: 1.3193955421447754\n","Epoch: 497 | Generator Loss: 0.7132951617240906 | Discriminator Loss: 1.3496453762054443\n","Epoch: 498 | Generator Loss: 0.7275828123092651 | Discriminator Loss: 1.3476767539978027\n","Epoch: 499 | Generator Loss: 0.6567821502685547 | Discriminator Loss: 1.2786725759506226\n","Epoch: 500 | Generator Loss: 0.7646530270576477 | Discriminator Loss: 1.299509882926941\n","Epoch: 501 | Generator Loss: 0.7694940567016602 | Discriminator Loss: 1.3381597995758057\n","Epoch: 502 | Generator Loss: 0.7578107118606567 | Discriminator Loss: 1.267467737197876\n","Epoch: 503 | Generator Loss: 0.7368052005767822 | Discriminator Loss: 1.2545384168624878\n","Epoch: 504 | Generator Loss: 0.7062168121337891 | Discriminator Loss: 1.248225450515747\n","Epoch: 505 | Generator Loss: 0.7099353075027466 | Discriminator Loss: 1.2452895641326904\n","Epoch: 506 | Generator Loss: 0.7193170189857483 | Discriminator Loss: 1.317899465560913\n","Epoch: 507 | Generator Loss: 0.6877357363700867 | Discriminator Loss: 1.2406351566314697\n","Epoch: 508 | Generator Loss: 0.7099887728691101 | Discriminator Loss: 1.306350827217102\n","Epoch: 509 | Generator Loss: 0.7414737343788147 | Discriminator Loss: 1.337113380432129\n","Epoch: 510 | Generator Loss: 0.7109223008155823 | Discriminator Loss: 1.2765777111053467\n","Epoch: 511 | Generator Loss: 0.7420738935470581 | Discriminator Loss: 1.2890301942825317\n","Epoch: 512 | Generator Loss: 0.7517105937004089 | Discriminator Loss: 1.2649955749511719\n","Epoch: 513 | Generator Loss: 0.7017167806625366 | Discriminator Loss: 1.337220549583435\n","Epoch: 514 | Generator Loss: 0.791164755821228 | Discriminator Loss: 1.2863829135894775\n","Epoch: 515 | Generator Loss: 0.7448434829711914 | Discriminator Loss: 1.3200953006744385\n","Epoch: 516 | Generator Loss: 0.7454412579536438 | Discriminator Loss: 1.2775602340698242\n","Epoch: 517 | Generator Loss: 0.7636385560035706 | Discriminator Loss: 1.3827965259552002\n","Epoch: 518 | Generator Loss: 0.7655112147331238 | Discriminator Loss: 1.2497681379318237\n","Epoch: 519 | Generator Loss: 0.7087282538414001 | Discriminator Loss: 1.254887342453003\n","Epoch: 520 | Generator Loss: 0.7387971878051758 | Discriminator Loss: 1.3397197723388672\n","Epoch: 521 | Generator Loss: 0.7519363164901733 | Discriminator Loss: 1.350061058998108\n","Epoch: 522 | Generator Loss: 0.7098745107650757 | Discriminator Loss: 1.2147743701934814\n","Epoch: 523 | Generator Loss: 0.7324177026748657 | Discriminator Loss: 1.3338162899017334\n","Epoch: 524 | Generator Loss: 0.7373772263526917 | Discriminator Loss: 1.2784860134124756\n","Epoch: 525 | Generator Loss: 0.7900353670120239 | Discriminator Loss: 1.2310041189193726\n","Epoch: 526 | Generator Loss: 0.7258834838867188 | Discriminator Loss: 1.3690685033798218\n","Epoch: 527 | Generator Loss: 0.7544099688529968 | Discriminator Loss: 1.3153128623962402\n","Epoch: 528 | Generator Loss: 0.7078309655189514 | Discriminator Loss: 1.2957324981689453\n","Epoch: 529 | Generator Loss: 0.7192257642745972 | Discriminator Loss: 1.2864890098571777\n","Epoch: 530 | Generator Loss: 0.696181058883667 | Discriminator Loss: 1.313556432723999\n","Epoch: 531 | Generator Loss: 0.7837396860122681 | Discriminator Loss: 1.4081149101257324\n","Epoch: 532 | Generator Loss: 0.7724399566650391 | Discriminator Loss: 1.2616814374923706\n","Epoch: 533 | Generator Loss: 0.7214891910552979 | Discriminator Loss: 1.3283787965774536\n","Epoch: 534 | Generator Loss: 0.7212023735046387 | Discriminator Loss: 1.3058737516403198\n","Epoch: 535 | Generator Loss: 0.752120852470398 | Discriminator Loss: 1.301225185394287\n","Epoch: 536 | Generator Loss: 0.7582747936248779 | Discriminator Loss: 1.3303736448287964\n","Epoch: 537 | Generator Loss: 0.7197380065917969 | Discriminator Loss: 1.2940576076507568\n","Epoch: 538 | Generator Loss: 0.7442615628242493 | Discriminator Loss: 1.3411176204681396\n","Epoch: 539 | Generator Loss: 0.792555570602417 | Discriminator Loss: 1.2891035079956055\n","Epoch: 540 | Generator Loss: 0.7060959339141846 | Discriminator Loss: 1.325056791305542\n","Epoch: 541 | Generator Loss: 0.7692465782165527 | Discriminator Loss: 1.3427728414535522\n","Epoch: 542 | Generator Loss: 0.7193887233734131 | Discriminator Loss: 1.3454289436340332\n","Epoch: 543 | Generator Loss: 0.7108230590820312 | Discriminator Loss: 1.258110761642456\n","Epoch: 544 | Generator Loss: 0.7850219011306763 | Discriminator Loss: 1.3209344148635864\n","Epoch: 545 | Generator Loss: 0.733727216720581 | Discriminator Loss: 1.35929274559021\n","Epoch: 546 | Generator Loss: 0.7070098519325256 | Discriminator Loss: 1.2724745273590088\n","Epoch: 547 | Generator Loss: 0.7469463348388672 | Discriminator Loss: 1.2742438316345215\n","Epoch: 548 | Generator Loss: 0.7531124949455261 | Discriminator Loss: 1.3078852891921997\n","Epoch: 549 | Generator Loss: 0.7154148817062378 | Discriminator Loss: 1.2336697578430176\n","Epoch: 550 | Generator Loss: 0.7580220699310303 | Discriminator Loss: 1.2376521825790405\n","Epoch: 551 | Generator Loss: 0.7476117014884949 | Discriminator Loss: 1.2045705318450928\n","Epoch: 552 | Generator Loss: 0.7810983657836914 | Discriminator Loss: 1.3013629913330078\n","Epoch: 553 | Generator Loss: 0.7726618647575378 | Discriminator Loss: 1.3075101375579834\n","Epoch: 554 | Generator Loss: 0.7532634139060974 | Discriminator Loss: 1.3119959831237793\n","Epoch: 555 | Generator Loss: 0.7318696975708008 | Discriminator Loss: 1.3517863750457764\n","Epoch: 556 | Generator Loss: 0.7493804693222046 | Discriminator Loss: 1.4084270000457764\n","Epoch: 557 | Generator Loss: 0.7420358061790466 | Discriminator Loss: 1.2630311250686646\n","Epoch: 558 | Generator Loss: 0.7504924535751343 | Discriminator Loss: 1.3793377876281738\n","Epoch: 559 | Generator Loss: 0.7306917905807495 | Discriminator Loss: 1.2894394397735596\n","Epoch: 560 | Generator Loss: 0.7184266448020935 | Discriminator Loss: 1.304494857788086\n","Epoch: 561 | Generator Loss: 0.7386436462402344 | Discriminator Loss: 1.2984941005706787\n","Epoch: 562 | Generator Loss: 0.7735227346420288 | Discriminator Loss: 1.2943812608718872\n","Epoch: 563 | Generator Loss: 0.7583160996437073 | Discriminator Loss: 1.3400633335113525\n","Epoch: 564 | Generator Loss: 0.7650163173675537 | Discriminator Loss: 1.3383581638336182\n","Epoch: 565 | Generator Loss: 0.7313232421875 | Discriminator Loss: 1.2522454261779785\n","Epoch: 566 | Generator Loss: 0.7408595681190491 | Discriminator Loss: 1.2540422677993774\n","Epoch: 567 | Generator Loss: 0.7411722540855408 | Discriminator Loss: 1.3375064134597778\n","Epoch: 568 | Generator Loss: 0.749755859375 | Discriminator Loss: 1.3042314052581787\n","Epoch: 569 | Generator Loss: 0.725333034992218 | Discriminator Loss: 1.224341869354248\n","Epoch: 570 | Generator Loss: 0.781051754951477 | Discriminator Loss: 1.2880237102508545\n","Epoch: 571 | Generator Loss: 0.7419900894165039 | Discriminator Loss: 1.3262474536895752\n","Epoch: 572 | Generator Loss: 0.7282800078392029 | Discriminator Loss: 1.2923636436462402\n","Epoch: 573 | Generator Loss: 0.7623549103736877 | Discriminator Loss: 1.2566125392913818\n","Epoch: 574 | Generator Loss: 0.7400318384170532 | Discriminator Loss: 1.377267599105835\n","Epoch: 575 | Generator Loss: 0.7648524641990662 | Discriminator Loss: 1.2850496768951416\n","Epoch: 576 | Generator Loss: 0.7495303750038147 | Discriminator Loss: 1.2568182945251465\n","Epoch: 577 | Generator Loss: 0.7412407398223877 | Discriminator Loss: 1.3166077136993408\n","Epoch: 578 | Generator Loss: 0.7329976558685303 | Discriminator Loss: 1.2692019939422607\n","Epoch: 579 | Generator Loss: 0.7201114296913147 | Discriminator Loss: 1.2253756523132324\n","Epoch: 580 | Generator Loss: 0.7210550308227539 | Discriminator Loss: 1.2966549396514893\n","Epoch: 581 | Generator Loss: 0.7225248217582703 | Discriminator Loss: 1.2752485275268555\n","Epoch: 582 | Generator Loss: 0.7199383974075317 | Discriminator Loss: 1.1953842639923096\n","Epoch: 583 | Generator Loss: 0.7391536235809326 | Discriminator Loss: 1.279020071029663\n","Epoch: 584 | Generator Loss: 0.7275562286376953 | Discriminator Loss: 1.3150970935821533\n","Epoch: 585 | Generator Loss: 0.7491551637649536 | Discriminator Loss: 1.3192987442016602\n","Epoch: 586 | Generator Loss: 0.7721318006515503 | Discriminator Loss: 1.2823808193206787\n","Epoch: 587 | Generator Loss: 0.7840323448181152 | Discriminator Loss: 1.2772235870361328\n","Epoch: 588 | Generator Loss: 0.7846239805221558 | Discriminator Loss: 1.3289320468902588\n","Epoch: 589 | Generator Loss: 0.7851748466491699 | Discriminator Loss: 1.306359052658081\n","Epoch: 590 | Generator Loss: 0.723311722278595 | Discriminator Loss: 1.2801992893218994\n","Epoch: 591 | Generator Loss: 0.7577174305915833 | Discriminator Loss: 1.3393276929855347\n","Epoch: 592 | Generator Loss: 0.7276950478553772 | Discriminator Loss: 1.3426185846328735\n","Epoch: 593 | Generator Loss: 0.727133572101593 | Discriminator Loss: 1.3558759689331055\n","Epoch: 594 | Generator Loss: 0.7180588841438293 | Discriminator Loss: 1.363897681236267\n","Epoch: 595 | Generator Loss: 0.7075839042663574 | Discriminator Loss: 1.3844479322433472\n","Epoch: 596 | Generator Loss: 0.7730658054351807 | Discriminator Loss: 1.3276149034500122\n","Epoch: 597 | Generator Loss: 0.7694293856620789 | Discriminator Loss: 1.3504533767700195\n","Epoch: 598 | Generator Loss: 0.7881717681884766 | Discriminator Loss: 1.3079533576965332\n","Epoch: 599 | Generator Loss: 0.7666187882423401 | Discriminator Loss: 1.324289321899414\n","Epoch: 600 | Generator Loss: 0.7563136219978333 | Discriminator Loss: 1.29417085647583\n","Epoch: 601 | Generator Loss: 0.7281028032302856 | Discriminator Loss: 1.3458161354064941\n","Epoch: 602 | Generator Loss: 0.759584903717041 | Discriminator Loss: 1.401692271232605\n","Epoch: 603 | Generator Loss: 0.7180147767066956 | Discriminator Loss: 1.2744476795196533\n","Epoch: 604 | Generator Loss: 0.7761974930763245 | Discriminator Loss: 1.298288345336914\n","Epoch: 605 | Generator Loss: 0.7205817103385925 | Discriminator Loss: 1.3030202388763428\n","Epoch: 606 | Generator Loss: 0.7586864829063416 | Discriminator Loss: 1.3378708362579346\n","Epoch: 607 | Generator Loss: 0.732918918132782 | Discriminator Loss: 1.288559913635254\n","Epoch: 608 | Generator Loss: 0.7035679817199707 | Discriminator Loss: 1.2641828060150146\n","Epoch: 609 | Generator Loss: 0.7365898489952087 | Discriminator Loss: 1.2674481868743896\n","Epoch: 610 | Generator Loss: 0.7154244184494019 | Discriminator Loss: 1.3192484378814697\n","Epoch: 611 | Generator Loss: 0.7726003527641296 | Discriminator Loss: 1.3122484683990479\n","Epoch: 612 | Generator Loss: 0.7701287865638733 | Discriminator Loss: 1.3722305297851562\n","Epoch: 613 | Generator Loss: 0.8061840534210205 | Discriminator Loss: 1.340694546699524\n","Epoch: 614 | Generator Loss: 0.7721930742263794 | Discriminator Loss: 1.2840461730957031\n","Epoch: 615 | Generator Loss: 0.7505754232406616 | Discriminator Loss: 1.3155150413513184\n","Epoch: 616 | Generator Loss: 0.8052371740341187 | Discriminator Loss: 1.2529497146606445\n","Epoch: 617 | Generator Loss: 0.7541634440422058 | Discriminator Loss: 1.212167739868164\n","Epoch: 618 | Generator Loss: 0.7205300331115723 | Discriminator Loss: 1.2517483234405518\n","Epoch: 619 | Generator Loss: 0.7241852283477783 | Discriminator Loss: 1.249572992324829\n","Epoch: 620 | Generator Loss: 0.744156002998352 | Discriminator Loss: 1.311094045639038\n","Epoch: 621 | Generator Loss: 0.7281281352043152 | Discriminator Loss: 1.2276453971862793\n","Epoch: 622 | Generator Loss: 0.7171646952629089 | Discriminator Loss: 1.2535820007324219\n","Epoch: 623 | Generator Loss: 0.7434267401695251 | Discriminator Loss: 1.25373375415802\n","Epoch: 624 | Generator Loss: 0.7221863865852356 | Discriminator Loss: 1.3729417324066162\n","Epoch: 625 | Generator Loss: 0.7482615113258362 | Discriminator Loss: 1.3072762489318848\n","Epoch: 626 | Generator Loss: 0.776111900806427 | Discriminator Loss: 1.2775354385375977\n","Epoch: 627 | Generator Loss: 0.7128353118896484 | Discriminator Loss: 1.2456345558166504\n","Epoch: 628 | Generator Loss: 0.7229510545730591 | Discriminator Loss: 1.2783164978027344\n","Epoch: 629 | Generator Loss: 0.7441240549087524 | Discriminator Loss: 1.36968994140625\n","Epoch: 630 | Generator Loss: 0.7276992797851562 | Discriminator Loss: 1.2292598485946655\n","Epoch: 631 | Generator Loss: 0.7680544257164001 | Discriminator Loss: 1.2296295166015625\n","Epoch: 632 | Generator Loss: 0.7127615213394165 | Discriminator Loss: 1.3516829013824463\n","Epoch: 633 | Generator Loss: 0.7207884788513184 | Discriminator Loss: 1.3126287460327148\n","Epoch: 634 | Generator Loss: 0.7799396514892578 | Discriminator Loss: 1.2719988822937012\n","Epoch: 635 | Generator Loss: 0.7469624280929565 | Discriminator Loss: 1.2823607921600342\n","Epoch: 636 | Generator Loss: 0.7493008375167847 | Discriminator Loss: 1.3594133853912354\n","Epoch: 637 | Generator Loss: 0.7261732220649719 | Discriminator Loss: 1.288084864616394\n","Epoch: 638 | Generator Loss: 0.7401294708251953 | Discriminator Loss: 1.2257812023162842\n","Epoch: 639 | Generator Loss: 0.7428296804428101 | Discriminator Loss: 1.2752314805984497\n","Epoch: 640 | Generator Loss: 0.7519591450691223 | Discriminator Loss: 1.2850295305252075\n","Epoch: 641 | Generator Loss: 0.7345864772796631 | Discriminator Loss: 1.209477424621582\n","Epoch: 642 | Generator Loss: 0.7920469045639038 | Discriminator Loss: 1.2380058765411377\n","Epoch: 643 | Generator Loss: 0.7526131868362427 | Discriminator Loss: 1.3732969760894775\n","Epoch: 644 | Generator Loss: 0.733104944229126 | Discriminator Loss: 1.2638466358184814\n","Epoch: 645 | Generator Loss: 0.7578586935997009 | Discriminator Loss: 1.273756504058838\n","Epoch: 646 | Generator Loss: 0.7692307233810425 | Discriminator Loss: 1.3060578107833862\n","Epoch: 647 | Generator Loss: 0.712605357170105 | Discriminator Loss: 1.2347052097320557\n","Epoch: 648 | Generator Loss: 0.7491316795349121 | Discriminator Loss: 1.3929893970489502\n","Epoch: 649 | Generator Loss: 0.7505406141281128 | Discriminator Loss: 1.2744824886322021\n","Epoch: 650 | Generator Loss: 0.7287905216217041 | Discriminator Loss: 1.283703088760376\n","Epoch: 651 | Generator Loss: 0.7300102114677429 | Discriminator Loss: 1.2629616260528564\n","Epoch: 652 | Generator Loss: 0.734465479850769 | Discriminator Loss: 1.2894316911697388\n","Epoch: 653 | Generator Loss: 0.7367734909057617 | Discriminator Loss: 1.3595333099365234\n","Epoch: 654 | Generator Loss: 0.7368861436843872 | Discriminator Loss: 1.2761152982711792\n","Epoch: 655 | Generator Loss: 0.7294120788574219 | Discriminator Loss: 1.2642216682434082\n","Epoch: 656 | Generator Loss: 0.6938130855560303 | Discriminator Loss: 1.2986072301864624\n","Epoch: 657 | Generator Loss: 0.7136250734329224 | Discriminator Loss: 1.214211344718933\n","Epoch: 658 | Generator Loss: 0.7426145076751709 | Discriminator Loss: 1.302178144454956\n","Epoch: 659 | Generator Loss: 0.7264240980148315 | Discriminator Loss: 1.2586665153503418\n","Epoch: 660 | Generator Loss: 0.7600113153457642 | Discriminator Loss: 1.2429397106170654\n","Epoch: 661 | Generator Loss: 0.788257360458374 | Discriminator Loss: 1.1817318201065063\n","Epoch: 662 | Generator Loss: 0.7281827926635742 | Discriminator Loss: 1.2001025676727295\n","Epoch: 663 | Generator Loss: 0.7201085686683655 | Discriminator Loss: 1.2742679119110107\n","Epoch: 664 | Generator Loss: 0.7549457550048828 | Discriminator Loss: 1.347530484199524\n","Epoch: 665 | Generator Loss: 0.7513923645019531 | Discriminator Loss: 1.2102980613708496\n","Epoch: 666 | Generator Loss: 0.7405730485916138 | Discriminator Loss: 1.2896380424499512\n","Epoch: 667 | Generator Loss: 0.7355688810348511 | Discriminator Loss: 1.321159839630127\n","Epoch: 668 | Generator Loss: 0.7232338786125183 | Discriminator Loss: 1.2876219749450684\n","Epoch: 669 | Generator Loss: 0.7490643262863159 | Discriminator Loss: 1.31535005569458\n","Epoch: 670 | Generator Loss: 0.7335102558135986 | Discriminator Loss: 1.3357518911361694\n","Epoch: 671 | Generator Loss: 0.7010973691940308 | Discriminator Loss: 1.3937523365020752\n","Epoch: 672 | Generator Loss: 0.7800239324569702 | Discriminator Loss: 1.2711089849472046\n","Epoch: 673 | Generator Loss: 0.7451424598693848 | Discriminator Loss: 1.3061788082122803\n","Epoch: 674 | Generator Loss: 0.7899969816207886 | Discriminator Loss: 1.2871469259262085\n","Epoch: 675 | Generator Loss: 0.7312314510345459 | Discriminator Loss: 1.2994208335876465\n","Epoch: 676 | Generator Loss: 0.7617027163505554 | Discriminator Loss: 1.3289353847503662\n","Epoch: 677 | Generator Loss: 0.7661477327346802 | Discriminator Loss: 1.2144770622253418\n","Epoch: 678 | Generator Loss: 0.7397375106811523 | Discriminator Loss: 1.2172445058822632\n","Epoch: 679 | Generator Loss: 0.7603287100791931 | Discriminator Loss: 1.3050200939178467\n","Epoch: 680 | Generator Loss: 0.7655559182167053 | Discriminator Loss: 1.2710952758789062\n","Epoch: 681 | Generator Loss: 0.7182763814926147 | Discriminator Loss: 1.3132075071334839\n","Epoch: 682 | Generator Loss: 0.7100063562393188 | Discriminator Loss: 1.3820102214813232\n","Epoch: 683 | Generator Loss: 0.7125679850578308 | Discriminator Loss: 1.2831552028656006\n","Epoch: 684 | Generator Loss: 0.7043172121047974 | Discriminator Loss: 1.2802047729492188\n","Epoch: 685 | Generator Loss: 0.7502167224884033 | Discriminator Loss: 1.2962521314620972\n","Epoch: 686 | Generator Loss: 0.7487432956695557 | Discriminator Loss: 1.4237654209136963\n","Epoch: 687 | Generator Loss: 0.7782468795776367 | Discriminator Loss: 1.2686835527420044\n","Epoch: 688 | Generator Loss: 0.7301032543182373 | Discriminator Loss: 1.338008165359497\n","Epoch: 689 | Generator Loss: 0.7349100112915039 | Discriminator Loss: 1.2395875453948975\n","Epoch: 690 | Generator Loss: 0.7177811861038208 | Discriminator Loss: 1.2623820304870605\n","Epoch: 691 | Generator Loss: 0.7009410858154297 | Discriminator Loss: 1.2911620140075684\n","Epoch: 692 | Generator Loss: 0.7240939140319824 | Discriminator Loss: 1.296736478805542\n","Epoch: 693 | Generator Loss: 0.7217082977294922 | Discriminator Loss: 1.3343656063079834\n","Epoch: 694 | Generator Loss: 0.7577541470527649 | Discriminator Loss: 1.4099738597869873\n","Epoch: 695 | Generator Loss: 0.7495700120925903 | Discriminator Loss: 1.2006760835647583\n","Epoch: 696 | Generator Loss: 0.7335787415504456 | Discriminator Loss: 1.271026849746704\n","Epoch: 697 | Generator Loss: 0.76099693775177 | Discriminator Loss: 1.3881921768188477\n","Epoch: 698 | Generator Loss: 0.6906814575195312 | Discriminator Loss: 1.3116812705993652\n","Epoch: 699 | Generator Loss: 0.7122888565063477 | Discriminator Loss: 1.3059239387512207\n","Epoch: 700 | Generator Loss: 0.7690446376800537 | Discriminator Loss: 1.22487473487854\n","Epoch: 701 | Generator Loss: 0.7706273794174194 | Discriminator Loss: 1.1821727752685547\n","Epoch: 702 | Generator Loss: 0.7164667844772339 | Discriminator Loss: 1.3105907440185547\n","Epoch: 703 | Generator Loss: 0.7246310710906982 | Discriminator Loss: 1.2524149417877197\n","Epoch: 704 | Generator Loss: 0.755036473274231 | Discriminator Loss: 1.1802078485488892\n","Epoch: 705 | Generator Loss: 0.7713433504104614 | Discriminator Loss: 1.2771923542022705\n","Epoch: 706 | Generator Loss: 0.7056158781051636 | Discriminator Loss: 1.2102208137512207\n","Epoch: 707 | Generator Loss: 0.7230949997901917 | Discriminator Loss: 1.3635612726211548\n","Epoch: 708 | Generator Loss: 0.7423946261405945 | Discriminator Loss: 1.4034252166748047\n","Epoch: 709 | Generator Loss: 0.7410314679145813 | Discriminator Loss: 1.2112936973571777\n","Epoch: 710 | Generator Loss: 0.7373733520507812 | Discriminator Loss: 1.3152782917022705\n","Epoch: 711 | Generator Loss: 0.720366358757019 | Discriminator Loss: 1.3815743923187256\n","Epoch: 712 | Generator Loss: 0.7465377449989319 | Discriminator Loss: 1.3201391696929932\n","Epoch: 713 | Generator Loss: 0.7554678916931152 | Discriminator Loss: 1.3154394626617432\n","Epoch: 714 | Generator Loss: 0.7612764835357666 | Discriminator Loss: 1.271831750869751\n","Epoch: 715 | Generator Loss: 0.7244973182678223 | Discriminator Loss: 1.25050950050354\n","Epoch: 716 | Generator Loss: 0.70600825548172 | Discriminator Loss: 1.309587001800537\n","Epoch: 717 | Generator Loss: 0.7353929281234741 | Discriminator Loss: 1.3117725849151611\n","Epoch: 718 | Generator Loss: 0.7580089569091797 | Discriminator Loss: 1.305781602859497\n","Epoch: 719 | Generator Loss: 0.7567138671875 | Discriminator Loss: 1.2826907634735107\n","Epoch: 720 | Generator Loss: 0.7142632007598877 | Discriminator Loss: 1.2963175773620605\n","Epoch: 721 | Generator Loss: 0.7327783703804016 | Discriminator Loss: 1.262342929840088\n","Epoch: 722 | Generator Loss: 0.7279959321022034 | Discriminator Loss: 1.3241547346115112\n","Epoch: 723 | Generator Loss: 0.761458158493042 | Discriminator Loss: 1.2824976444244385\n","Epoch: 724 | Generator Loss: 0.7163916826248169 | Discriminator Loss: 1.2282249927520752\n","Epoch: 725 | Generator Loss: 0.7303942441940308 | Discriminator Loss: 1.375274896621704\n","Epoch: 726 | Generator Loss: 0.7421211004257202 | Discriminator Loss: 1.2631562948226929\n","Epoch: 727 | Generator Loss: 0.7481586933135986 | Discriminator Loss: 1.1679375171661377\n","Epoch: 728 | Generator Loss: 0.7517324686050415 | Discriminator Loss: 1.2663284540176392\n","Epoch: 729 | Generator Loss: 0.7153834104537964 | Discriminator Loss: 1.3645403385162354\n","Epoch: 730 | Generator Loss: 0.7384492754936218 | Discriminator Loss: 1.0658743381500244\n","Epoch: 731 | Generator Loss: 0.7106702923774719 | Discriminator Loss: 1.2759182453155518\n","Epoch: 732 | Generator Loss: 0.6947231292724609 | Discriminator Loss: 1.2695627212524414\n","Epoch: 733 | Generator Loss: 0.7264927625656128 | Discriminator Loss: 1.233821153640747\n","Epoch: 734 | Generator Loss: 0.7081836462020874 | Discriminator Loss: 1.315937876701355\n","Epoch: 735 | Generator Loss: 0.7717366814613342 | Discriminator Loss: 1.322862148284912\n","Epoch: 736 | Generator Loss: 0.7602444887161255 | Discriminator Loss: 1.291899561882019\n","Epoch: 737 | Generator Loss: 0.7198169231414795 | Discriminator Loss: 1.3423585891723633\n","Epoch: 738 | Generator Loss: 0.7420576810836792 | Discriminator Loss: 1.2612204551696777\n","Epoch: 739 | Generator Loss: 0.7023493051528931 | Discriminator Loss: 1.3810296058654785\n","Epoch: 740 | Generator Loss: 0.7973337173461914 | Discriminator Loss: 1.2991666793823242\n","Epoch: 741 | Generator Loss: 0.7212074995040894 | Discriminator Loss: 1.191591739654541\n","Epoch: 742 | Generator Loss: 0.7990636825561523 | Discriminator Loss: 1.2754863500595093\n","Epoch: 743 | Generator Loss: 0.7375428676605225 | Discriminator Loss: 1.2119637727737427\n","Epoch: 744 | Generator Loss: 0.7042697072029114 | Discriminator Loss: 1.1643122434616089\n","Epoch: 745 | Generator Loss: 0.7475289702415466 | Discriminator Loss: 1.229067087173462\n","Epoch: 746 | Generator Loss: 0.7324202060699463 | Discriminator Loss: 1.2651320695877075\n","Epoch: 747 | Generator Loss: 0.7360243797302246 | Discriminator Loss: 1.276918888092041\n","Epoch: 748 | Generator Loss: 0.7307276725769043 | Discriminator Loss: 1.16058349609375\n","Epoch: 749 | Generator Loss: 0.7432824373245239 | Discriminator Loss: 1.2962356805801392\n","Epoch: 750 | Generator Loss: 0.7513926029205322 | Discriminator Loss: 1.360907793045044\n","Epoch: 751 | Generator Loss: 0.7436490058898926 | Discriminator Loss: 1.2397756576538086\n","Epoch: 752 | Generator Loss: 0.7473374605178833 | Discriminator Loss: 1.3660006523132324\n","Epoch: 753 | Generator Loss: 0.7069036960601807 | Discriminator Loss: 1.2470341920852661\n","Epoch: 754 | Generator Loss: 0.7419639825820923 | Discriminator Loss: 1.3139293193817139\n","Epoch: 755 | Generator Loss: 0.7176017165184021 | Discriminator Loss: 1.2491989135742188\n","Epoch: 756 | Generator Loss: 0.7444499135017395 | Discriminator Loss: 1.2428431510925293\n","Epoch: 757 | Generator Loss: 0.7123094797134399 | Discriminator Loss: 1.3400707244873047\n","Epoch: 758 | Generator Loss: 0.7388723492622375 | Discriminator Loss: 1.2793476581573486\n","Epoch: 759 | Generator Loss: 0.6827998161315918 | Discriminator Loss: 1.3000767230987549\n","Epoch: 760 | Generator Loss: 0.7982763648033142 | Discriminator Loss: 1.3322147130966187\n","Epoch: 761 | Generator Loss: 0.7810878157615662 | Discriminator Loss: 1.2842817306518555\n","Epoch: 762 | Generator Loss: 0.7118151187896729 | Discriminator Loss: 1.250091314315796\n","Epoch: 763 | Generator Loss: 0.7634203433990479 | Discriminator Loss: 1.3163130283355713\n","Epoch: 764 | Generator Loss: 0.7054890990257263 | Discriminator Loss: 1.2223354578018188\n","Epoch: 765 | Generator Loss: 0.7408151626586914 | Discriminator Loss: 1.230722427368164\n","Epoch: 766 | Generator Loss: 0.7873722314834595 | Discriminator Loss: 1.1904118061065674\n","Epoch: 767 | Generator Loss: 0.7344706058502197 | Discriminator Loss: 1.288650393486023\n","Epoch: 768 | Generator Loss: 0.7243629693984985 | Discriminator Loss: 1.2355495691299438\n","Epoch: 769 | Generator Loss: 0.7558702826499939 | Discriminator Loss: 1.263920545578003\n","Epoch: 770 | Generator Loss: 0.7885921001434326 | Discriminator Loss: 1.2541393041610718\n","Epoch: 771 | Generator Loss: 0.754126787185669 | Discriminator Loss: 1.303805947303772\n","Epoch: 772 | Generator Loss: 0.7225555181503296 | Discriminator Loss: 1.2345952987670898\n","Epoch: 773 | Generator Loss: 0.7312626838684082 | Discriminator Loss: 1.2183520793914795\n","Epoch: 774 | Generator Loss: 0.7593966722488403 | Discriminator Loss: 1.3037066459655762\n","Epoch: 775 | Generator Loss: 0.7524638175964355 | Discriminator Loss: 1.2350420951843262\n","Epoch: 776 | Generator Loss: 0.7849642038345337 | Discriminator Loss: 1.2174856662750244\n","Epoch: 777 | Generator Loss: 0.7458644509315491 | Discriminator Loss: 1.2621281147003174\n","Epoch: 778 | Generator Loss: 0.7784871459007263 | Discriminator Loss: 1.2326734066009521\n","Epoch: 779 | Generator Loss: 0.7745733261108398 | Discriminator Loss: 1.2181212902069092\n","Epoch: 780 | Generator Loss: 0.6896631717681885 | Discriminator Loss: 1.2830760478973389\n","Epoch: 781 | Generator Loss: 0.718076229095459 | Discriminator Loss: 1.2609807252883911\n","Epoch: 782 | Generator Loss: 0.6980698108673096 | Discriminator Loss: 1.2784976959228516\n","Epoch: 783 | Generator Loss: 0.7643851041793823 | Discriminator Loss: 1.3306152820587158\n","Epoch: 784 | Generator Loss: 0.7691709995269775 | Discriminator Loss: 1.22233247756958\n","Epoch: 785 | Generator Loss: 0.7328310012817383 | Discriminator Loss: 1.184754490852356\n","Epoch: 786 | Generator Loss: 0.7227957248687744 | Discriminator Loss: 1.2110399007797241\n","Epoch: 787 | Generator Loss: 0.8004124164581299 | Discriminator Loss: 1.2147818803787231\n","Epoch: 788 | Generator Loss: 0.7203645706176758 | Discriminator Loss: 1.3460570573806763\n","Epoch: 789 | Generator Loss: 0.6961879134178162 | Discriminator Loss: 1.2876960039138794\n","Epoch: 790 | Generator Loss: 0.7524983882904053 | Discriminator Loss: 1.3102985620498657\n","Epoch: 791 | Generator Loss: 0.7667475938796997 | Discriminator Loss: 1.1956437826156616\n","Epoch: 792 | Generator Loss: 0.7515910863876343 | Discriminator Loss: 1.3347760438919067\n","Epoch: 793 | Generator Loss: 0.7428258657455444 | Discriminator Loss: 1.292801856994629\n","Epoch: 794 | Generator Loss: 0.7511200308799744 | Discriminator Loss: 1.1684086322784424\n","Epoch: 795 | Generator Loss: 0.7714325189590454 | Discriminator Loss: 1.3517284393310547\n","Epoch: 796 | Generator Loss: 0.7490022778511047 | Discriminator Loss: 1.187739610671997\n","Epoch: 797 | Generator Loss: 0.7810546159744263 | Discriminator Loss: 1.3439648151397705\n","Epoch: 798 | Generator Loss: 0.7185344696044922 | Discriminator Loss: 1.2817285060882568\n","Epoch: 799 | Generator Loss: 0.7808931469917297 | Discriminator Loss: 1.2760766744613647\n","Epoch: 800 | Generator Loss: 0.7184212803840637 | Discriminator Loss: 1.291843295097351\n","Epoch: 801 | Generator Loss: 0.7071365714073181 | Discriminator Loss: 1.2759885787963867\n","Epoch: 802 | Generator Loss: 0.7768853306770325 | Discriminator Loss: 1.157909870147705\n","Epoch: 803 | Generator Loss: 0.7385727167129517 | Discriminator Loss: 1.2123229503631592\n","Epoch: 804 | Generator Loss: 0.7246530652046204 | Discriminator Loss: 1.250098705291748\n","Epoch: 805 | Generator Loss: 0.7736216187477112 | Discriminator Loss: 1.2025494575500488\n","Epoch: 806 | Generator Loss: 0.7142324447631836 | Discriminator Loss: 1.3205503225326538\n","Epoch: 807 | Generator Loss: 0.6992142200469971 | Discriminator Loss: 1.2777999639511108\n","Epoch: 808 | Generator Loss: 0.7283742427825928 | Discriminator Loss: 1.3450379371643066\n","Epoch: 809 | Generator Loss: 0.6844368577003479 | Discriminator Loss: 1.2936307191848755\n","Epoch: 810 | Generator Loss: 0.7077288031578064 | Discriminator Loss: 1.2540321350097656\n","Epoch: 811 | Generator Loss: 0.7600841522216797 | Discriminator Loss: 1.323171615600586\n","Epoch: 812 | Generator Loss: 0.7459031343460083 | Discriminator Loss: 1.2716985940933228\n","Epoch: 813 | Generator Loss: 0.7010211944580078 | Discriminator Loss: 1.2321921586990356\n","Epoch: 814 | Generator Loss: 0.7530229091644287 | Discriminator Loss: 1.2030832767486572\n","Epoch: 815 | Generator Loss: 0.7408613562583923 | Discriminator Loss: 1.375999093055725\n","Epoch: 816 | Generator Loss: 0.7380102872848511 | Discriminator Loss: 1.3601980209350586\n","Epoch: 817 | Generator Loss: 0.7595624923706055 | Discriminator Loss: 1.2177469730377197\n","Epoch: 818 | Generator Loss: 0.7122430205345154 | Discriminator Loss: 1.0998139381408691\n","Epoch: 819 | Generator Loss: 0.7590263485908508 | Discriminator Loss: 1.155564308166504\n","Epoch: 820 | Generator Loss: 0.7357110977172852 | Discriminator Loss: 1.2101325988769531\n","Epoch: 821 | Generator Loss: 0.708056628704071 | Discriminator Loss: 1.2433233261108398\n","Epoch: 822 | Generator Loss: 0.7569286823272705 | Discriminator Loss: 1.1897413730621338\n","Epoch: 823 | Generator Loss: 0.7329639196395874 | Discriminator Loss: 1.267600655555725\n","Epoch: 824 | Generator Loss: 0.7193042039871216 | Discriminator Loss: 1.2539294958114624\n","Epoch: 825 | Generator Loss: 0.7652267813682556 | Discriminator Loss: 1.3722212314605713\n","Epoch: 826 | Generator Loss: 0.7647057771682739 | Discriminator Loss: 1.3018946647644043\n","Epoch: 827 | Generator Loss: 0.7662034034729004 | Discriminator Loss: 1.3109357357025146\n","Epoch: 828 | Generator Loss: 0.7386085391044617 | Discriminator Loss: 1.2055811882019043\n","Epoch: 829 | Generator Loss: 0.6844021081924438 | Discriminator Loss: 1.2267967462539673\n","Epoch: 830 | Generator Loss: 0.7656506299972534 | Discriminator Loss: 1.312191367149353\n","Epoch: 831 | Generator Loss: 0.7190730571746826 | Discriminator Loss: 1.2383780479431152\n","Epoch: 832 | Generator Loss: 0.7809843420982361 | Discriminator Loss: 1.2889137268066406\n","Epoch: 833 | Generator Loss: 0.7070596218109131 | Discriminator Loss: 1.2515952587127686\n","Epoch: 834 | Generator Loss: 0.7281357645988464 | Discriminator Loss: 1.193666934967041\n","Epoch: 835 | Generator Loss: 0.7322971820831299 | Discriminator Loss: 1.1468669176101685\n","Epoch: 836 | Generator Loss: 0.7245898842811584 | Discriminator Loss: 1.3238296508789062\n","Epoch: 837 | Generator Loss: 0.7217322587966919 | Discriminator Loss: 1.311580777168274\n","Epoch: 838 | Generator Loss: 0.7588670253753662 | Discriminator Loss: 1.2808277606964111\n","Epoch: 839 | Generator Loss: 0.7385969758033752 | Discriminator Loss: 1.2227566242218018\n","Epoch: 840 | Generator Loss: 0.6994374990463257 | Discriminator Loss: 1.2481632232666016\n","Epoch: 841 | Generator Loss: 0.7627334594726562 | Discriminator Loss: 1.2901930809020996\n","Epoch: 842 | Generator Loss: 0.7750837802886963 | Discriminator Loss: 1.2293787002563477\n","Epoch: 843 | Generator Loss: 0.7349733710289001 | Discriminator Loss: 1.1646862030029297\n","Epoch: 844 | Generator Loss: 0.6988278031349182 | Discriminator Loss: 1.332533836364746\n","Epoch: 845 | Generator Loss: 0.7547540664672852 | Discriminator Loss: 1.2242636680603027\n","Epoch: 846 | Generator Loss: 0.740711510181427 | Discriminator Loss: 1.254613995552063\n","Epoch: 847 | Generator Loss: 0.777725338935852 | Discriminator Loss: 1.2498571872711182\n","Epoch: 848 | Generator Loss: 0.7330234050750732 | Discriminator Loss: 1.2501153945922852\n","Epoch: 849 | Generator Loss: 0.7342987060546875 | Discriminator Loss: 1.3049895763397217\n","Epoch: 850 | Generator Loss: 0.779767632484436 | Discriminator Loss: 1.257957935333252\n","Epoch: 851 | Generator Loss: 0.7613620162010193 | Discriminator Loss: 1.0941078662872314\n","Epoch: 852 | Generator Loss: 0.7139477729797363 | Discriminator Loss: 1.3052160739898682\n","Epoch: 853 | Generator Loss: 0.7392674684524536 | Discriminator Loss: 1.2212049961090088\n","Epoch: 854 | Generator Loss: 0.7633366584777832 | Discriminator Loss: 1.2648804187774658\n","Epoch: 855 | Generator Loss: 0.7597450017929077 | Discriminator Loss: 1.3208427429199219\n","Epoch: 856 | Generator Loss: 0.7438013553619385 | Discriminator Loss: 1.2979800701141357\n","Epoch: 857 | Generator Loss: 0.6844789981842041 | Discriminator Loss: 1.255164623260498\n","Epoch: 858 | Generator Loss: 0.7791678309440613 | Discriminator Loss: 1.242621660232544\n","Epoch: 859 | Generator Loss: 0.7524174451828003 | Discriminator Loss: 1.2777056694030762\n","Epoch: 860 | Generator Loss: 0.7180173397064209 | Discriminator Loss: 1.2657915353775024\n","Epoch: 861 | Generator Loss: 0.7298868894577026 | Discriminator Loss: 1.258790135383606\n","Epoch: 862 | Generator Loss: 0.6944252252578735 | Discriminator Loss: 1.2391486167907715\n","Epoch: 863 | Generator Loss: 0.713064968585968 | Discriminator Loss: 1.1989352703094482\n","Epoch: 864 | Generator Loss: 0.72291100025177 | Discriminator Loss: 1.3084417581558228\n","Epoch: 865 | Generator Loss: 0.7206317186355591 | Discriminator Loss: 1.2354029417037964\n","Epoch: 866 | Generator Loss: 0.7394037842750549 | Discriminator Loss: 1.2603211402893066\n","Epoch: 867 | Generator Loss: 0.7408500909805298 | Discriminator Loss: 1.3024353981018066\n","Epoch: 868 | Generator Loss: 0.6879533529281616 | Discriminator Loss: 1.3435702323913574\n","Epoch: 869 | Generator Loss: 0.747104287147522 | Discriminator Loss: 1.2348625659942627\n","Epoch: 870 | Generator Loss: 0.753367006778717 | Discriminator Loss: 1.2256865501403809\n","Epoch: 871 | Generator Loss: 0.7187092304229736 | Discriminator Loss: 1.2451345920562744\n","Epoch: 872 | Generator Loss: 0.7673574090003967 | Discriminator Loss: 1.1021440029144287\n","Epoch: 873 | Generator Loss: 0.7951221466064453 | Discriminator Loss: 1.252936601638794\n","Epoch: 874 | Generator Loss: 0.7117546796798706 | Discriminator Loss: 1.3566584587097168\n","Epoch: 875 | Generator Loss: 0.7569077014923096 | Discriminator Loss: 1.2408229112625122\n","Epoch: 876 | Generator Loss: 0.7021505236625671 | Discriminator Loss: 1.2947938442230225\n","Epoch: 877 | Generator Loss: 0.7343637943267822 | Discriminator Loss: 1.148960828781128\n","Epoch: 878 | Generator Loss: 0.7432149648666382 | Discriminator Loss: 1.243816614151001\n","Epoch: 879 | Generator Loss: 0.7557547092437744 | Discriminator Loss: 1.3852784633636475\n","Epoch: 880 | Generator Loss: 0.7639291286468506 | Discriminator Loss: 1.2316745519638062\n","Epoch: 881 | Generator Loss: 0.7347123026847839 | Discriminator Loss: 1.1481428146362305\n","Epoch: 882 | Generator Loss: 0.7401766777038574 | Discriminator Loss: 1.1979597806930542\n","Epoch: 883 | Generator Loss: 0.8130694627761841 | Discriminator Loss: 1.2189488410949707\n","Epoch: 884 | Generator Loss: 0.7631542682647705 | Discriminator Loss: 1.2221649885177612\n","Epoch: 885 | Generator Loss: 0.7440285086631775 | Discriminator Loss: 1.237871766090393\n","Epoch: 886 | Generator Loss: 0.7174122333526611 | Discriminator Loss: 1.2678771018981934\n","Epoch: 887 | Generator Loss: 0.7307778000831604 | Discriminator Loss: 1.2772122621536255\n","Epoch: 888 | Generator Loss: 0.7557719945907593 | Discriminator Loss: 1.2696077823638916\n","Epoch: 889 | Generator Loss: 0.7695068717002869 | Discriminator Loss: 1.2457623481750488\n","Epoch: 890 | Generator Loss: 0.7621496915817261 | Discriminator Loss: 1.1915454864501953\n","Epoch: 891 | Generator Loss: 0.7034634351730347 | Discriminator Loss: 1.2709882259368896\n","Epoch: 892 | Generator Loss: 0.7405856251716614 | Discriminator Loss: 1.3184378147125244\n","Epoch: 893 | Generator Loss: 0.7180095911026001 | Discriminator Loss: 1.2649160623550415\n","Epoch: 894 | Generator Loss: 0.7198662757873535 | Discriminator Loss: 1.224151372909546\n","Epoch: 895 | Generator Loss: 0.7248260378837585 | Discriminator Loss: 1.169553279876709\n","Epoch: 896 | Generator Loss: 0.758579432964325 | Discriminator Loss: 1.2609648704528809\n","Epoch: 897 | Generator Loss: 0.7126452922821045 | Discriminator Loss: 1.272400140762329\n","Epoch: 898 | Generator Loss: 0.731664776802063 | Discriminator Loss: 1.2741506099700928\n","Epoch: 899 | Generator Loss: 0.7326095104217529 | Discriminator Loss: 1.3039565086364746\n","Epoch: 900 | Generator Loss: 0.7302155494689941 | Discriminator Loss: 1.2206288576126099\n","Epoch: 901 | Generator Loss: 0.7505230903625488 | Discriminator Loss: 1.3431843519210815\n","Epoch: 902 | Generator Loss: 0.737596869468689 | Discriminator Loss: 1.3172190189361572\n","Epoch: 903 | Generator Loss: 0.704653263092041 | Discriminator Loss: 1.3231874704360962\n","Epoch: 904 | Generator Loss: 0.6712503433227539 | Discriminator Loss: 1.1919939517974854\n","Epoch: 905 | Generator Loss: 0.7486911416053772 | Discriminator Loss: 1.2187167406082153\n","Epoch: 906 | Generator Loss: 0.7302068471908569 | Discriminator Loss: 1.2200536727905273\n","Epoch: 907 | Generator Loss: 0.7258732318878174 | Discriminator Loss: 1.2204809188842773\n","Epoch: 908 | Generator Loss: 0.7410328388214111 | Discriminator Loss: 1.3158988952636719\n","Epoch: 909 | Generator Loss: 0.7428078055381775 | Discriminator Loss: 1.3087847232818604\n","Epoch: 910 | Generator Loss: 0.7380856275558472 | Discriminator Loss: 1.3704721927642822\n","Epoch: 911 | Generator Loss: 0.7761083245277405 | Discriminator Loss: 1.3421293497085571\n","Epoch: 912 | Generator Loss: 0.7428693771362305 | Discriminator Loss: 1.3069275617599487\n","Epoch: 913 | Generator Loss: 0.7570128440856934 | Discriminator Loss: 1.2622243165969849\n","Epoch: 914 | Generator Loss: 0.7056673765182495 | Discriminator Loss: 1.2956390380859375\n","Epoch: 915 | Generator Loss: 0.7977548241615295 | Discriminator Loss: 1.2046079635620117\n","Epoch: 916 | Generator Loss: 0.7781844139099121 | Discriminator Loss: 1.3294131755828857\n","Epoch: 917 | Generator Loss: 0.7977420091629028 | Discriminator Loss: 1.186694860458374\n","Epoch: 918 | Generator Loss: 0.7320210933685303 | Discriminator Loss: 1.2748253345489502\n","Epoch: 919 | Generator Loss: 0.7262156009674072 | Discriminator Loss: 1.2022370100021362\n","Epoch: 920 | Generator Loss: 0.6785050630569458 | Discriminator Loss: 1.1737916469573975\n","Epoch: 921 | Generator Loss: 0.7240644693374634 | Discriminator Loss: 1.3405156135559082\n","Epoch: 922 | Generator Loss: 0.778874933719635 | Discriminator Loss: 1.119199275970459\n","Epoch: 923 | Generator Loss: 0.7481416463851929 | Discriminator Loss: 1.164607286453247\n","Epoch: 924 | Generator Loss: 0.7508751153945923 | Discriminator Loss: 1.3150410652160645\n","Epoch: 925 | Generator Loss: 0.6960767507553101 | Discriminator Loss: 1.36130952835083\n","Epoch: 926 | Generator Loss: 0.789840579032898 | Discriminator Loss: 1.2279188632965088\n","Epoch: 927 | Generator Loss: 0.7771162986755371 | Discriminator Loss: 1.278907060623169\n","Epoch: 928 | Generator Loss: 0.7424677610397339 | Discriminator Loss: 1.2039811611175537\n","Epoch: 929 | Generator Loss: 0.7604021430015564 | Discriminator Loss: 1.1912131309509277\n","Epoch: 930 | Generator Loss: 0.7903859615325928 | Discriminator Loss: 1.28279709815979\n","Epoch: 931 | Generator Loss: 0.7415790557861328 | Discriminator Loss: 1.269709587097168\n","Epoch: 932 | Generator Loss: 0.6671850681304932 | Discriminator Loss: 1.3197089433670044\n","Epoch: 933 | Generator Loss: 0.7283812165260315 | Discriminator Loss: 1.230492353439331\n","Epoch: 934 | Generator Loss: 0.7945100665092468 | Discriminator Loss: 1.2327697277069092\n","Epoch: 935 | Generator Loss: 0.7396386861801147 | Discriminator Loss: 1.2561819553375244\n","Epoch: 936 | Generator Loss: 0.7860007286071777 | Discriminator Loss: 1.3026821613311768\n","Epoch: 937 | Generator Loss: 0.732400119304657 | Discriminator Loss: 1.2767558097839355\n","Epoch: 938 | Generator Loss: 0.7694965600967407 | Discriminator Loss: 1.253492832183838\n","Epoch: 939 | Generator Loss: 0.7172640562057495 | Discriminator Loss: 1.2469546794891357\n","Epoch: 940 | Generator Loss: 0.7577562928199768 | Discriminator Loss: 1.208876609802246\n","Epoch: 941 | Generator Loss: 0.7034763097763062 | Discriminator Loss: 1.223363995552063\n","Epoch: 942 | Generator Loss: 0.7769395709037781 | Discriminator Loss: 1.272720217704773\n","Epoch: 943 | Generator Loss: 0.7510184049606323 | Discriminator Loss: 1.2139790058135986\n","Epoch: 944 | Generator Loss: 0.7133470773696899 | Discriminator Loss: 1.2331563234329224\n","Epoch: 945 | Generator Loss: 0.7795042991638184 | Discriminator Loss: 1.2542996406555176\n","Epoch: 946 | Generator Loss: 0.7144432067871094 | Discriminator Loss: 1.3500422239303589\n","Epoch: 947 | Generator Loss: 0.7802822589874268 | Discriminator Loss: 1.1646220684051514\n","Epoch: 948 | Generator Loss: 0.7068923711776733 | Discriminator Loss: 1.238662838935852\n","Epoch: 949 | Generator Loss: 0.752246618270874 | Discriminator Loss: 1.2425134181976318\n","Epoch: 950 | Generator Loss: 0.7288041114807129 | Discriminator Loss: 1.099496841430664\n","Epoch: 951 | Generator Loss: 0.81397944688797 | Discriminator Loss: 1.2561969757080078\n","Epoch: 952 | Generator Loss: 0.7581112384796143 | Discriminator Loss: 1.194896936416626\n","Epoch: 953 | Generator Loss: 0.7278048992156982 | Discriminator Loss: 1.2269129753112793\n","Epoch: 954 | Generator Loss: 0.7541587352752686 | Discriminator Loss: 1.2969509363174438\n","Epoch: 955 | Generator Loss: 0.7223978638648987 | Discriminator Loss: 1.2689664363861084\n","Epoch: 956 | Generator Loss: 0.7797163128852844 | Discriminator Loss: 1.2869943380355835\n","Epoch: 957 | Generator Loss: 0.7462484836578369 | Discriminator Loss: 1.1513288021087646\n","Epoch: 958 | Generator Loss: 0.735480785369873 | Discriminator Loss: 1.273512363433838\n","Epoch: 959 | Generator Loss: 0.7528402805328369 | Discriminator Loss: 1.2520602941513062\n","Epoch: 960 | Generator Loss: 0.7183466553688049 | Discriminator Loss: 1.1937649250030518\n","Epoch: 961 | Generator Loss: 0.757946252822876 | Discriminator Loss: 1.2689083814620972\n","Epoch: 962 | Generator Loss: 0.7614195346832275 | Discriminator Loss: 1.2631962299346924\n","Epoch: 963 | Generator Loss: 0.7286785244941711 | Discriminator Loss: 1.3662185668945312\n","Epoch: 964 | Generator Loss: 0.7551196813583374 | Discriminator Loss: 1.21047043800354\n","Epoch: 965 | Generator Loss: 0.7570338845252991 | Discriminator Loss: 1.1987080574035645\n","Epoch: 966 | Generator Loss: 0.7045537829399109 | Discriminator Loss: 1.2894046306610107\n","Epoch: 967 | Generator Loss: 0.7281820178031921 | Discriminator Loss: 1.2105544805526733\n","Epoch: 968 | Generator Loss: 0.7143551111221313 | Discriminator Loss: 1.2075607776641846\n","Epoch: 969 | Generator Loss: 0.6945850849151611 | Discriminator Loss: 1.3471732139587402\n","Epoch: 970 | Generator Loss: 0.7378572821617126 | Discriminator Loss: 1.2399824857711792\n","Epoch: 971 | Generator Loss: 0.7990458011627197 | Discriminator Loss: 1.2526018619537354\n","Epoch: 972 | Generator Loss: 0.7621244192123413 | Discriminator Loss: 1.3023697137832642\n","Epoch: 973 | Generator Loss: 0.7520816326141357 | Discriminator Loss: 1.208868145942688\n","Epoch: 974 | Generator Loss: 0.7291030287742615 | Discriminator Loss: 1.138396143913269\n","Epoch: 975 | Generator Loss: 0.7643277645111084 | Discriminator Loss: 1.197934627532959\n","Epoch: 976 | Generator Loss: 0.7083628177642822 | Discriminator Loss: 1.3297815322875977\n","Epoch: 977 | Generator Loss: 0.7611759901046753 | Discriminator Loss: 1.2246066331863403\n","Epoch: 978 | Generator Loss: 0.7961726188659668 | Discriminator Loss: 1.2266364097595215\n","Epoch: 979 | Generator Loss: 0.7122408747673035 | Discriminator Loss: 1.2737163305282593\n","Epoch: 980 | Generator Loss: 0.7685196399688721 | Discriminator Loss: 1.3115742206573486\n","Epoch: 981 | Generator Loss: 0.7487701177597046 | Discriminator Loss: 1.237566590309143\n","Epoch: 982 | Generator Loss: 0.7089205980300903 | Discriminator Loss: 1.2131884098052979\n","Epoch: 983 | Generator Loss: 0.7371420860290527 | Discriminator Loss: 1.2495298385620117\n","Epoch: 984 | Generator Loss: 0.7028204798698425 | Discriminator Loss: 1.2463834285736084\n","Epoch: 985 | Generator Loss: 0.7469751834869385 | Discriminator Loss: 1.3460304737091064\n","Epoch: 986 | Generator Loss: 0.7663189172744751 | Discriminator Loss: 1.216742753982544\n","Epoch: 987 | Generator Loss: 0.7094995379447937 | Discriminator Loss: 1.2920646667480469\n","Epoch: 988 | Generator Loss: 0.7500230073928833 | Discriminator Loss: 1.243851900100708\n","Epoch: 989 | Generator Loss: 0.7797474265098572 | Discriminator Loss: 1.341510534286499\n","Epoch: 990 | Generator Loss: 0.7548012733459473 | Discriminator Loss: 1.2301366329193115\n","Epoch: 991 | Generator Loss: 0.7888022661209106 | Discriminator Loss: 1.1810386180877686\n","Epoch: 992 | Generator Loss: 0.7232919931411743 | Discriminator Loss: 1.2484815120697021\n","Epoch: 993 | Generator Loss: 0.7496733665466309 | Discriminator Loss: 1.288863182067871\n","Epoch: 994 | Generator Loss: 0.791808009147644 | Discriminator Loss: 1.1971344947814941\n","Epoch: 995 | Generator Loss: 0.7444051504135132 | Discriminator Loss: 1.147001028060913\n","Epoch: 996 | Generator Loss: 0.7421669960021973 | Discriminator Loss: 1.2377214431762695\n","Epoch: 997 | Generator Loss: 0.751656711101532 | Discriminator Loss: 1.3116848468780518\n","Epoch: 998 | Generator Loss: 0.7045397758483887 | Discriminator Loss: 1.2803912162780762\n","Epoch: 999 | Generator Loss: 0.7306618690490723 | Discriminator Loss: 1.1412019729614258\n","Epoch: 1000 | Generator Loss: 0.7333710789680481 | Discriminator Loss: 1.2255496978759766\n"]}]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available else 'cpu'\n","\n","# Mengenerate data sintetik sebanyak 5000 row dari random noise menggunakan\n","# Generator yang sudah ditrain\n","noise = torch.randn(5000, latent_noise).to(device)\n","generated_data = gan.generator(noise).to('cpu')\n","generated_data = generated_data.detach().numpy()\n","generated_data_df = pd.DataFrame(generated_data)\n","generated_data_df.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"EX9iZh6VA5se","executionInfo":{"status":"ok","timestamp":1686542671704,"user_tz":-420,"elapsed":32,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"5ff34e51-f00b-4164-a50b-dba0aeab3679"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                0            1            2            3            4   \\\n","count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n","mean      0.305642     0.016291     0.086594     0.038966     0.212711   \n","std       0.117409     0.052289     0.037856     0.045703     0.058776   \n","min       0.092367    -0.147534    -0.031817    -0.194990     0.047462   \n","25%       0.215331    -0.017553     0.063138     0.010936     0.175312   \n","50%       0.291023     0.012822     0.080005     0.045219     0.205200   \n","75%       0.377398     0.046638     0.103771     0.073149     0.245125   \n","max       0.814484     0.251262     0.359413     0.132292     0.590629   \n","\n","                5            6            7            8            9   \\\n","count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n","mean     -0.042049     0.032680     0.145826     0.209107    -0.122492   \n","std       0.115424     0.073159     0.086471     0.075960     0.033151   \n","min      -0.592373    -0.185574    -0.060949     0.033925    -0.263360   \n","25%      -0.104743    -0.019594     0.077723     0.152180    -0.140670   \n","50%      -0.015912     0.017164     0.135479     0.201937    -0.118180   \n","75%       0.039233     0.072636     0.201417     0.257349    -0.102867   \n","max       0.215448     0.401399     0.525220     0.580519     0.019473   \n","\n","                10           11           12  \n","count  5000.000000  5000.000000  5000.000000  \n","mean     -0.122330    -0.048662    -0.251433  \n","std       0.081034     0.047164     0.095688  \n","min      -0.402556    -0.240171    -0.602665  \n","25%      -0.177030    -0.075823    -0.316557  \n","50%      -0.121602    -0.049596    -0.240793  \n","75%      -0.073821    -0.021929    -0.179946  \n","max       0.219136     0.139820    -0.034373  "],"text/html":["\n","  <div id=\"df-86355eed-2785-4fae-a9b2-47399af21759\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.305642</td>\n","      <td>0.016291</td>\n","      <td>0.086594</td>\n","      <td>0.038966</td>\n","      <td>0.212711</td>\n","      <td>-0.042049</td>\n","      <td>0.032680</td>\n","      <td>0.145826</td>\n","      <td>0.209107</td>\n","      <td>-0.122492</td>\n","      <td>-0.122330</td>\n","      <td>-0.048662</td>\n","      <td>-0.251433</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.117409</td>\n","      <td>0.052289</td>\n","      <td>0.037856</td>\n","      <td>0.045703</td>\n","      <td>0.058776</td>\n","      <td>0.115424</td>\n","      <td>0.073159</td>\n","      <td>0.086471</td>\n","      <td>0.075960</td>\n","      <td>0.033151</td>\n","      <td>0.081034</td>\n","      <td>0.047164</td>\n","      <td>0.095688</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.092367</td>\n","      <td>-0.147534</td>\n","      <td>-0.031817</td>\n","      <td>-0.194990</td>\n","      <td>0.047462</td>\n","      <td>-0.592373</td>\n","      <td>-0.185574</td>\n","      <td>-0.060949</td>\n","      <td>0.033925</td>\n","      <td>-0.263360</td>\n","      <td>-0.402556</td>\n","      <td>-0.240171</td>\n","      <td>-0.602665</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.215331</td>\n","      <td>-0.017553</td>\n","      <td>0.063138</td>\n","      <td>0.010936</td>\n","      <td>0.175312</td>\n","      <td>-0.104743</td>\n","      <td>-0.019594</td>\n","      <td>0.077723</td>\n","      <td>0.152180</td>\n","      <td>-0.140670</td>\n","      <td>-0.177030</td>\n","      <td>-0.075823</td>\n","      <td>-0.316557</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.291023</td>\n","      <td>0.012822</td>\n","      <td>0.080005</td>\n","      <td>0.045219</td>\n","      <td>0.205200</td>\n","      <td>-0.015912</td>\n","      <td>0.017164</td>\n","      <td>0.135479</td>\n","      <td>0.201937</td>\n","      <td>-0.118180</td>\n","      <td>-0.121602</td>\n","      <td>-0.049596</td>\n","      <td>-0.240793</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.377398</td>\n","      <td>0.046638</td>\n","      <td>0.103771</td>\n","      <td>0.073149</td>\n","      <td>0.245125</td>\n","      <td>0.039233</td>\n","      <td>0.072636</td>\n","      <td>0.201417</td>\n","      <td>0.257349</td>\n","      <td>-0.102867</td>\n","      <td>-0.073821</td>\n","      <td>-0.021929</td>\n","      <td>-0.179946</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.814484</td>\n","      <td>0.251262</td>\n","      <td>0.359413</td>\n","      <td>0.132292</td>\n","      <td>0.590629</td>\n","      <td>0.215448</td>\n","      <td>0.401399</td>\n","      <td>0.525220</td>\n","      <td>0.580519</td>\n","      <td>0.019473</td>\n","      <td>0.219136</td>\n","      <td>0.139820</td>\n","      <td>-0.034373</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86355eed-2785-4fae-a9b2-47399af21759')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-86355eed-2785-4fae-a9b2-47399af21759 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-86355eed-2785-4fae-a9b2-47399af21759');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["pumpkin_data = pd.read_excel('/content/drive/MyDrive/Datasets/Pumpkin_Seeds_Dataset.xlsx')\n","\n","class_map = {'Çerçevelik': 0, 'Ürgüp Sivrisi':1}\n","pumpkin_data['Class'] = pumpkin_data['Class'].replace(class_map)\n","\n","real_data = pumpkin_data\n","real_data.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"RmN64I7VBupP","executionInfo":{"status":"ok","timestamp":1686542688617,"user_tz":-420,"elapsed":40,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"3a1963c8-7d2e-487f-a473-a521cf441d81"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                Area    Perimeter  Major_Axis_Length  Minor_Axis_Length  \\\n","count    2500.000000  2500.000000        2500.000000        2500.000000   \n","mean    80658.220800  1130.279015         456.601840         225.794921   \n","std     13664.510228   109.256418          56.235704          23.297245   \n","min     47939.000000   868.485000         320.844600         152.171800   \n","25%     70765.000000  1048.829750         414.957850         211.245925   \n","50%     79076.000000  1123.672000         449.496600         224.703100   \n","75%     89757.500000  1203.340500         492.737650         240.672875   \n","max    136574.000000  1559.450000         661.911300         305.818000   \n","\n","         Convex_Area  Equiv_Diameter  Eccentricity     Solidity       Extent  \\\n","count    2500.000000     2500.000000   2500.000000  2500.000000  2500.000000   \n","mean    81508.084400      319.334230      0.860879     0.989492     0.693205   \n","std     13764.092788       26.891920      0.045167     0.003494     0.060914   \n","min     48366.000000      247.058400      0.492100     0.918600     0.468000   \n","25%     71512.000000      300.167975      0.831700     0.988300     0.658900   \n","50%     79872.000000      317.305350      0.863700     0.990300     0.713050   \n","75%     90797.750000      338.057375      0.897025     0.991500     0.740225   \n","max    138384.000000      417.002900      0.948100     0.994400     0.829600   \n","\n","         Roundness  Aspect_Ration  Compactness      Class  \n","count  2500.000000    2500.000000  2500.000000  2500.0000  \n","mean      0.791533       2.041702     0.704121     0.4800  \n","std       0.055924       0.315997     0.053067     0.4997  \n","min       0.554600       1.148700     0.560800     0.0000  \n","25%       0.751900       1.801050     0.663475     0.0000  \n","50%       0.797750       1.984200     0.707700     0.0000  \n","75%       0.834325       2.262075     0.743500     1.0000  \n","max       0.939600       3.144400     0.904900     1.0000  "],"text/html":["\n","  <div id=\"df-251a2ead-7e8c-46b5-ade2-e9c4ba765d69\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Area</th>\n","      <th>Perimeter</th>\n","      <th>Major_Axis_Length</th>\n","      <th>Minor_Axis_Length</th>\n","      <th>Convex_Area</th>\n","      <th>Equiv_Diameter</th>\n","      <th>Eccentricity</th>\n","      <th>Solidity</th>\n","      <th>Extent</th>\n","      <th>Roundness</th>\n","      <th>Aspect_Ration</th>\n","      <th>Compactness</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.000000</td>\n","      <td>2500.0000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>80658.220800</td>\n","      <td>1130.279015</td>\n","      <td>456.601840</td>\n","      <td>225.794921</td>\n","      <td>81508.084400</td>\n","      <td>319.334230</td>\n","      <td>0.860879</td>\n","      <td>0.989492</td>\n","      <td>0.693205</td>\n","      <td>0.791533</td>\n","      <td>2.041702</td>\n","      <td>0.704121</td>\n","      <td>0.4800</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>13664.510228</td>\n","      <td>109.256418</td>\n","      <td>56.235704</td>\n","      <td>23.297245</td>\n","      <td>13764.092788</td>\n","      <td>26.891920</td>\n","      <td>0.045167</td>\n","      <td>0.003494</td>\n","      <td>0.060914</td>\n","      <td>0.055924</td>\n","      <td>0.315997</td>\n","      <td>0.053067</td>\n","      <td>0.4997</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>47939.000000</td>\n","      <td>868.485000</td>\n","      <td>320.844600</td>\n","      <td>152.171800</td>\n","      <td>48366.000000</td>\n","      <td>247.058400</td>\n","      <td>0.492100</td>\n","      <td>0.918600</td>\n","      <td>0.468000</td>\n","      <td>0.554600</td>\n","      <td>1.148700</td>\n","      <td>0.560800</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>70765.000000</td>\n","      <td>1048.829750</td>\n","      <td>414.957850</td>\n","      <td>211.245925</td>\n","      <td>71512.000000</td>\n","      <td>300.167975</td>\n","      <td>0.831700</td>\n","      <td>0.988300</td>\n","      <td>0.658900</td>\n","      <td>0.751900</td>\n","      <td>1.801050</td>\n","      <td>0.663475</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>79076.000000</td>\n","      <td>1123.672000</td>\n","      <td>449.496600</td>\n","      <td>224.703100</td>\n","      <td>79872.000000</td>\n","      <td>317.305350</td>\n","      <td>0.863700</td>\n","      <td>0.990300</td>\n","      <td>0.713050</td>\n","      <td>0.797750</td>\n","      <td>1.984200</td>\n","      <td>0.707700</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>89757.500000</td>\n","      <td>1203.340500</td>\n","      <td>492.737650</td>\n","      <td>240.672875</td>\n","      <td>90797.750000</td>\n","      <td>338.057375</td>\n","      <td>0.897025</td>\n","      <td>0.991500</td>\n","      <td>0.740225</td>\n","      <td>0.834325</td>\n","      <td>2.262075</td>\n","      <td>0.743500</td>\n","      <td>1.0000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>136574.000000</td>\n","      <td>1559.450000</td>\n","      <td>661.911300</td>\n","      <td>305.818000</td>\n","      <td>138384.000000</td>\n","      <td>417.002900</td>\n","      <td>0.948100</td>\n","      <td>0.994400</td>\n","      <td>0.829600</td>\n","      <td>0.939600</td>\n","      <td>3.144400</td>\n","      <td>0.904900</td>\n","      <td>1.0000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-251a2ead-7e8c-46b5-ade2-e9c4ba765d69')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-251a2ead-7e8c-46b5-ade2-e9c4ba765d69 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-251a2ead-7e8c-46b5-ade2-e9c4ba765d69');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["generated_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ca6Hvx2LC115","executionInfo":{"status":"ok","timestamp":1686542698278,"user_tz":-420,"elapsed":31,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"468b2d53-641d-4221-fca2-685ac8003371"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 13)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["generated_data = generated_data_df.values\n","\n","for i in range(generated_data.shape[1]):\n","  generated_data[:,i] = (generated_data[:,i] - generated_data[:,i].min())/(generated_data[:,i].max() - generated_data[:,i].min())\n","\n","generated_data = generated_data.astype('float32')"],"metadata":{"id":"FET82xW1C5MB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generated_df = pd.DataFrame(generated_data)\n","generated_df_numpy = generated_df.values\n","generated_df_numpy.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIfGiY-cCi5W","executionInfo":{"status":"ok","timestamp":1686542708004,"user_tz":-420,"elapsed":4,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"753f5bf3-f187-4ac1-8248-53102ec47666"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 13)"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["generated_df_numpy[:,12]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0g89URWyeOXk","executionInfo":{"status":"ok","timestamp":1686542711615,"user_tz":-420,"elapsed":1123,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"e9b5f758-c3c1-41c3-9449-caef5c9db9ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.6675177 , 0.14279106, 0.58377266, ..., 0.41775852, 0.3089065 ,\n","       0.5215311 ], dtype=float32)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# Denormalisasi data agar memiliki nilai dengan range mirip dengan data asli\n","\n","for idx, col in enumerate(real_data.columns):\n","  max = real_data[col].max()\n","  min = real_data[col].min()\n","\n","  if col != 'Class' and col != 'Area':\n","    generated_df_numpy[:, idx] = generated_df_numpy[:, idx]*(max - min) + min\n","    generated_df_numpy = generated_df_numpy[generated_df_numpy[:, idx] >= min]\n","\n","  elif col == 'Class':\n","    X = generated_df_numpy[:, idx]\n","    X_norm = []\n","    for X_i in X:\n","      X_i = 0 if X_i < 0.5 else 1\n","      X_norm.append(X_i)\n","    X_norm = np.array(X_norm)\n","    generated_df_numpy[:, idx] = X_norm\n","\n","  elif col == 'Area':\n","    X = generated_df_numpy[:, idx]\n","    X_norm = []\n","    for X_i in X:\n","      X_i = X_i * (max - min) + min\n","      X_norm.append(X_i)\n","    X_norm = np.array(X_norm)\n","    generated_df_numpy[:, idx] = X_norm\n","\n","new_df = pd.DataFrame(generated_df_numpy)\n","new_df.describe()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"xizp6IR5Z2kC","executionInfo":{"status":"ok","timestamp":1686542713650,"user_tz":-420,"elapsed":8,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"2ff74c02-2ef9-4eee-d0be-125aa4f94aa0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                  0            1            2            3              4   \\\n","count    5000.000000  5000.000000  5000.000000  5000.000000    5000.000000   \n","mean    74117.023438  1152.331543   424.072998   262.005096   75752.382812   \n","std     14411.225586    90.596733    33.002201    21.455608    9740.819336   \n","min     47939.000000   868.484985   320.844604   152.171799   48366.000000   \n","25%     63031.967773  1093.693512   403.623978   248.845806   69554.451172   \n","50%     72322.703125  1146.322205   418.328979   264.940659   74507.648438   \n","75%     82924.666016  1204.911926   439.047653   278.052536   81124.312500   \n","max    136574.000000  1559.449951   661.911316   305.817993  138384.000000   \n","\n","                5            6            7            8            9   \\\n","count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n","mean    362.832336     0.661654     0.945339     0.583892     0.746354   \n","std      24.282143     0.056834     0.011182     0.050252     0.045125   \n","min     247.058395     0.492100     0.918600     0.468000     0.554600   \n","25%     349.643082     0.621045     0.936532     0.546232     0.721610   \n","50%     368.330765     0.649601     0.944001     0.579149     0.752223   \n","75%     379.931793     0.692695     0.952528     0.615807     0.773067   \n","max     417.002899     0.948100     0.994400     0.829600     0.939600   \n","\n","                10           11           12  \n","count  5000.000000  5000.000000  5000.000000  \n","mean      2.048255     0.734220     0.754200  \n","std       0.260128     0.042710     0.430604  \n","min       1.148700     0.560800     0.000000  \n","25%       1.872664     0.709624     1.000000  \n","50%       2.050594     0.733374     1.000000  \n","75%       2.203974     0.758428     1.000000  \n","max       3.144400     0.904900     1.000000  "],"text/html":["\n","  <div id=\"df-a7e511aa-1922-4497-830c-7bbf6fb2d805\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","      <td>5000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>74117.023438</td>\n","      <td>1152.331543</td>\n","      <td>424.072998</td>\n","      <td>262.005096</td>\n","      <td>75752.382812</td>\n","      <td>362.832336</td>\n","      <td>0.661654</td>\n","      <td>0.945339</td>\n","      <td>0.583892</td>\n","      <td>0.746354</td>\n","      <td>2.048255</td>\n","      <td>0.734220</td>\n","      <td>0.754200</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>14411.225586</td>\n","      <td>90.596733</td>\n","      <td>33.002201</td>\n","      <td>21.455608</td>\n","      <td>9740.819336</td>\n","      <td>24.282143</td>\n","      <td>0.056834</td>\n","      <td>0.011182</td>\n","      <td>0.050252</td>\n","      <td>0.045125</td>\n","      <td>0.260128</td>\n","      <td>0.042710</td>\n","      <td>0.430604</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>47939.000000</td>\n","      <td>868.484985</td>\n","      <td>320.844604</td>\n","      <td>152.171799</td>\n","      <td>48366.000000</td>\n","      <td>247.058395</td>\n","      <td>0.492100</td>\n","      <td>0.918600</td>\n","      <td>0.468000</td>\n","      <td>0.554600</td>\n","      <td>1.148700</td>\n","      <td>0.560800</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>63031.967773</td>\n","      <td>1093.693512</td>\n","      <td>403.623978</td>\n","      <td>248.845806</td>\n","      <td>69554.451172</td>\n","      <td>349.643082</td>\n","      <td>0.621045</td>\n","      <td>0.936532</td>\n","      <td>0.546232</td>\n","      <td>0.721610</td>\n","      <td>1.872664</td>\n","      <td>0.709624</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>72322.703125</td>\n","      <td>1146.322205</td>\n","      <td>418.328979</td>\n","      <td>264.940659</td>\n","      <td>74507.648438</td>\n","      <td>368.330765</td>\n","      <td>0.649601</td>\n","      <td>0.944001</td>\n","      <td>0.579149</td>\n","      <td>0.752223</td>\n","      <td>2.050594</td>\n","      <td>0.733374</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>82924.666016</td>\n","      <td>1204.911926</td>\n","      <td>439.047653</td>\n","      <td>278.052536</td>\n","      <td>81124.312500</td>\n","      <td>379.931793</td>\n","      <td>0.692695</td>\n","      <td>0.952528</td>\n","      <td>0.615807</td>\n","      <td>0.773067</td>\n","      <td>2.203974</td>\n","      <td>0.758428</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>136574.000000</td>\n","      <td>1559.449951</td>\n","      <td>661.911316</td>\n","      <td>305.817993</td>\n","      <td>138384.000000</td>\n","      <td>417.002899</td>\n","      <td>0.948100</td>\n","      <td>0.994400</td>\n","      <td>0.829600</td>\n","      <td>0.939600</td>\n","      <td>3.144400</td>\n","      <td>0.904900</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7e511aa-1922-4497-830c-7bbf6fb2d805')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a7e511aa-1922-4497-830c-7bbf6fb2d805 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a7e511aa-1922-4497-830c-7bbf6fb2d805');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["new_df[[0, 4, 12]] = new_df[[0, 4, 12]].astype(int)\n","\n","new_df.columns = real_data.columns\n","new_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"Z42dy8vSfi2o","executionInfo":{"status":"ok","timestamp":1686542718692,"user_tz":-420,"elapsed":8,"user":{"displayName":"SST Archange","userId":"00687597079602900259"}},"outputId":"323a5a44-ffbf-41f0-b77e-96595740849d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Area    Perimeter  Major_Axis_Length  Minor_Axis_Length  Convex_Area  \\\n","0     55730  1155.412231         378.056366         263.222534        64748   \n","1     81850  1203.990967         564.665527         265.642395       113399   \n","2     56691  1230.360596         371.063477         244.080109        71541   \n","3     62410  1367.269531         415.794891         233.070724        80367   \n","4     59028  1090.964844         385.268921         254.597992        64327   \n","...     ...          ...                ...                ...          ...   \n","4995  70415  1155.803589         418.090210         289.292908        76340   \n","4996  90195  1296.499878         458.215851         276.466553        83805   \n","4997  72713  1071.256348         479.345764         212.254517        68820   \n","4998  76544  1172.845581         432.220978         227.433502        75017   \n","4999  77824  1029.497559         388.963989         248.388275        60308   \n","\n","      Equiv_Diameter  Eccentricity  Solidity    Extent  Roundness  \\\n","0         377.582764      0.627790  0.937673  0.514988   0.760821   \n","1         398.524017      0.666381  0.953716  0.723836   0.791000   \n","2         373.999298      0.629373  0.946049  0.583005   0.750483   \n","3         351.725372      0.677522  0.970288  0.644715   0.702836   \n","4         380.620056      0.644424  0.931928  0.527692   0.785164   \n","...              ...           ...       ...       ...        ...   \n","4995      370.242432      0.602830  0.930423  0.535034   0.760331   \n","4996      321.628723      0.651667  0.942690  0.581542   0.687192   \n","4997      356.394806      0.696356  0.937712  0.605875   0.740397   \n","4998      349.184204      0.586717  0.944879  0.568123   0.740454   \n","4999      377.520691      0.737149  0.949102  0.602484   0.744349   \n","\n","      Aspect_Ration  Compactness  Class  \n","0          2.058789     0.754934      1  \n","1          2.833417     0.863898      0  \n","2          2.138185     0.791028      1  \n","3          2.291118     0.743717      1  \n","4          2.070366     0.755354      1  \n","...             ...          ...    ...  \n","4995       1.810571     0.741103      1  \n","4996       1.626000     0.683104      1  \n","4997       2.792032     0.737174      0  \n","4998       2.236784     0.762664      0  \n","4999       2.160608     0.739347      1  \n","\n","[5000 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-4f988055-13ec-4a7e-a9f1-acbdecac25b5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Area</th>\n","      <th>Perimeter</th>\n","      <th>Major_Axis_Length</th>\n","      <th>Minor_Axis_Length</th>\n","      <th>Convex_Area</th>\n","      <th>Equiv_Diameter</th>\n","      <th>Eccentricity</th>\n","      <th>Solidity</th>\n","      <th>Extent</th>\n","      <th>Roundness</th>\n","      <th>Aspect_Ration</th>\n","      <th>Compactness</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>55730</td>\n","      <td>1155.412231</td>\n","      <td>378.056366</td>\n","      <td>263.222534</td>\n","      <td>64748</td>\n","      <td>377.582764</td>\n","      <td>0.627790</td>\n","      <td>0.937673</td>\n","      <td>0.514988</td>\n","      <td>0.760821</td>\n","      <td>2.058789</td>\n","      <td>0.754934</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>81850</td>\n","      <td>1203.990967</td>\n","      <td>564.665527</td>\n","      <td>265.642395</td>\n","      <td>113399</td>\n","      <td>398.524017</td>\n","      <td>0.666381</td>\n","      <td>0.953716</td>\n","      <td>0.723836</td>\n","      <td>0.791000</td>\n","      <td>2.833417</td>\n","      <td>0.863898</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>56691</td>\n","      <td>1230.360596</td>\n","      <td>371.063477</td>\n","      <td>244.080109</td>\n","      <td>71541</td>\n","      <td>373.999298</td>\n","      <td>0.629373</td>\n","      <td>0.946049</td>\n","      <td>0.583005</td>\n","      <td>0.750483</td>\n","      <td>2.138185</td>\n","      <td>0.791028</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>62410</td>\n","      <td>1367.269531</td>\n","      <td>415.794891</td>\n","      <td>233.070724</td>\n","      <td>80367</td>\n","      <td>351.725372</td>\n","      <td>0.677522</td>\n","      <td>0.970288</td>\n","      <td>0.644715</td>\n","      <td>0.702836</td>\n","      <td>2.291118</td>\n","      <td>0.743717</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>59028</td>\n","      <td>1090.964844</td>\n","      <td>385.268921</td>\n","      <td>254.597992</td>\n","      <td>64327</td>\n","      <td>380.620056</td>\n","      <td>0.644424</td>\n","      <td>0.931928</td>\n","      <td>0.527692</td>\n","      <td>0.785164</td>\n","      <td>2.070366</td>\n","      <td>0.755354</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4995</th>\n","      <td>70415</td>\n","      <td>1155.803589</td>\n","      <td>418.090210</td>\n","      <td>289.292908</td>\n","      <td>76340</td>\n","      <td>370.242432</td>\n","      <td>0.602830</td>\n","      <td>0.930423</td>\n","      <td>0.535034</td>\n","      <td>0.760331</td>\n","      <td>1.810571</td>\n","      <td>0.741103</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4996</th>\n","      <td>90195</td>\n","      <td>1296.499878</td>\n","      <td>458.215851</td>\n","      <td>276.466553</td>\n","      <td>83805</td>\n","      <td>321.628723</td>\n","      <td>0.651667</td>\n","      <td>0.942690</td>\n","      <td>0.581542</td>\n","      <td>0.687192</td>\n","      <td>1.626000</td>\n","      <td>0.683104</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4997</th>\n","      <td>72713</td>\n","      <td>1071.256348</td>\n","      <td>479.345764</td>\n","      <td>212.254517</td>\n","      <td>68820</td>\n","      <td>356.394806</td>\n","      <td>0.696356</td>\n","      <td>0.937712</td>\n","      <td>0.605875</td>\n","      <td>0.740397</td>\n","      <td>2.792032</td>\n","      <td>0.737174</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4998</th>\n","      <td>76544</td>\n","      <td>1172.845581</td>\n","      <td>432.220978</td>\n","      <td>227.433502</td>\n","      <td>75017</td>\n","      <td>349.184204</td>\n","      <td>0.586717</td>\n","      <td>0.944879</td>\n","      <td>0.568123</td>\n","      <td>0.740454</td>\n","      <td>2.236784</td>\n","      <td>0.762664</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4999</th>\n","      <td>77824</td>\n","      <td>1029.497559</td>\n","      <td>388.963989</td>\n","      <td>248.388275</td>\n","      <td>60308</td>\n","      <td>377.520691</td>\n","      <td>0.737149</td>\n","      <td>0.949102</td>\n","      <td>0.602484</td>\n","      <td>0.744349</td>\n","      <td>2.160608</td>\n","      <td>0.739347</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 13 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f988055-13ec-4a7e-a9f1-acbdecac25b5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4f988055-13ec-4a7e-a9f1-acbdecac25b5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4f988055-13ec-4a7e-a9f1-acbdecac25b5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# menyimpan data sintetik ke dalam format csv agar dapat diconsume oleh model\n","# Machine Learning yang lain.\n","\n","new_df.to_csv('/content/drive/MyDrive/Datasets/5000_Pumpkin_Seeds_GAN_Dataset.csv', index=False)"],"metadata":{"id":"lXd0B_gWjlD6"},"execution_count":null,"outputs":[]}]}